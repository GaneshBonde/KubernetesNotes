Github Link:
https://github.com/kodekloudhub/certified-kubernetes-administrator-course

CLI tools:-  ctr, nerdctl,  crictl (CRI compatiable container)

nerdctl better than ctr tools.

crictl can used for different container runtimes eg Docker, rkt

kubernetes is a container orchestrator
Open container initiate: 
						imagespec: on how image should be built
						runtimespec: on how the container should run at runtime.

containerD is a industry standard container runtime with emphasis on on simplicity , robustness and portability.

images pull docker.io/library/redis:alpine

run docker.io/library/redis:alpine redis

run --name webserver -p 80:80 -d nginx 

-----------------------------------------------------------------------------------------------------------------
ETCD is a distributed reliable key-value store that is Simple , Secure and Fast.


ETCD - Commands (Optional)

(Optional) Additional information about ETCDCTL Utility

ETCDCTL is the CLI tool used to interact with ETCD.

ETCDCTL can interact with ETCD Server using 2 API versions - Version 2 and Version 3.  By default its set to use Version 2. Each version has different sets of commands.

For example ETCDCTL version 2 supports the following commands:

    etcdctl backup
    etcdctl cluster-health
    etcdctl mk
    etcdctl mkdir
    etcdctl set


Whereas the commands are different in version 3

    etcdctl snapshot save 
    etcdctl endpoint health
    etcdctl get
    etcdctl put

To set the right version of API set the environment variable ETCDCTL_API command


export ETCDCTL_API=3


When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.


Apart from that, you must also specify path to certificate files so that ETCDCTL can authenticate to the ETCD API Server. The certificate files are available in the etcd-master at the following path. We discuss more about certificates in the security section of this course. So don't worry if this looks complex:

    --cacert /etc/kubernetes/pki/etcd/ca.crt     
    --cert /etc/kubernetes/pki/etcd/server.crt     
    --key /etc/kubernetes/pki/etcd/server.key


So for the commands I showed in the previous video to work you must specify the ETCDCTL API version and path to certificate files. Below is the final form:


    kubectl exec etcd-master -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl get / --prefix --keys-only --limit=10 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt  --key /etc/kubernetes/pki/etcd/server.key" 
	

*******************************************************************************************************************************************************************************
Yml main properties for Kubernetes. Contains four mandatory fields. It is case sensitive.
apiVersion:    	mandatory properties
kind:       	mandatory properties
metadata:		mandatory properties
spec: 			mandatory properties


*******************************************************************************************************************************************************************************
apiVersion: V1  -----------------vedrsion of the file
kind: Pod     	-----------------kind of object to be created/deployed/changed  (eg: Pod, Service, ReplicaSet and Deployment)
metadata: 		----------------- information about the object created/deployed/changed.
	name: myapp-pod
	labels: 
		app: myapp
		type: front-end
spec: 
	containers:
		- name: nginx-container
		  image: nginx
		  
commands:
kubectl create -f pod-defintion.xml  -- to create pod
kubectl get pods  -- to get list of running pods
kubectl describe pod myapp-pod  --- to get detailed information about the pod.


vim pod.yml    to create file
wq!  to save and close
cat pod.yml to view the yml 
*******************************************************************************************************************************************************************************
**** sample yml file to create a pod with two containers
apiVersion: V1
kind: Pod
metadata: 
	name: nginx
	labels:
		app: nginx
		tier: frontend
spec:
	containers:
	- name: nginx
	  image: nginx
	- name: busybox
	  image:busybox	
		
*******************************************************************************************************************************************************************************
kubectl apply -f pod.yml                      ---- creates new pod
kubectl get pods                              ---- get details of all pods
kubectl describe pod nginx

https://uklabs.kodecloud.com/topic/practice-test-pods-2


kubectl get svc
course 28: 
kubectl get pods -o wide

kubectl run redis --image=redis123 --dry-run=client -o yaml > redis.yml

kubectl create -f redis.yml

vi redis.yml

kubectl apply -f redis.yml

*******************************************************************************************************************************************************************************

Replication Controller vs Replica Set

replicationController-defintion.xml

apiVersion: v1
kind: ReplicationController
metadata:
	name: myapp-replicationController
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels: 
				app: myapp
				type: front-end
		spec:
			containers:
			-	name: nginx-container
				image: nginx
replicas: 3

kubectl create -f replicationController-defintion.yml

kubectl get replicationcontroller
***************************************************
replicaSet-defintion.yml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: myapp-replicaset
	labels: 
		app: myapp
		type: front-end
spec:
	template:
		metadata: 
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec: 
			containers:
			- 	name: ngnix-controller
				image: nginx
replicas: 3
selector:
	matchLabels: 
		type: front-end
		
kubectl create -f replicaset-defintion.yml		

kubectl get replicaset

kubectl replace -f replicaset-defintion.yml

kubectl scale --replicas=6 -f replicaset-defintion.yml

kubectl scale --replicas=6 replicaset myapp-replicaset

kubectl delete replicaset myapp-replicaset

kubectl delete pod pod-name

kubectl delete rs replicaset-1

kubectl delete replicaset replicaset-1

kubectl edit rs new-replicaset

kubectl scale rs new-replicaset --replicas=5

*******************************************************************************************************************************************************************************
deployment-defintion.yml

apiVersion: apps/v1
kind: Deployment
metadata: 
	name: Deployment
	labels: 
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-pod
			labels:
				app: myapp
				type: front-end
		spec:
			containers:
			- 	name: nginx-container
				image: nginx
	replicas: 3
	selector:
		matchLabels:
			type: front-end

kubectl create -f deployment-defintion.yml

kubectl get deployments

kubectl get replicaset

kubectl run nginx --image=nginx

kubectl run nginx --image=nginx --dry-run=client -o yaml 

kubectl create deployment --image=nginx nginx 

kubectl create deployment --image=nginx --dry-run=client -o yaml 

kubectl create deployemtn --image=nginx --dry-run=client -o yaml > nginx-controller.yaml

kubectl create -f nginx-deployment.yml

kubectl create deployment --image-nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yml

*******************************************************************************************************************************************************************************
Kubernetes Service
Service Types:
1) Node Port service (30000-32767) Node port ranage
2) ClusterIP service
3) LoadBalancer service

service-definition.yml (Node Port)

apiVersion: v1
kind: Service
metadata:
	name: myapp-service
spec: 
	type: NodePort
	ports:
	-	targetPort: 80                          (not mandatory. when not mentioned, it is same as port)
		port: 80                                (mandatory field required)
		nodePort: 3008 							(if not mentioned than random port is assigned from 30000-32767)
	selector:
		app: myapp	
		type: front-end
		
kubectl create -f service-definition.yml

kubectl get services

*******************************************************************************************************************************************************************************
service-definition.yml

apiVersion: v1
kind: Service
metadata: 
	name: myapp-service
spec: 
	type: LoadBalancer
	ports:
	- 	targetPort: 80
		port: 80
		nodePort: 30008
		

kubectl get service

kubectl get svc 

*******************************************************************************************************************************************************************************
mysql.connect("db-service")
mysql.connect("db-service.dev.svc.cluster.local")
			 Service Name	  Service	Domain
						Namespace

kubectl get pods

kubectl get pods --namespace=kube-system

kubectl create -f pod-defintion.yml --namespace=dev

apiVersion: v1
kind: Pod

metadata: 
	name: myapp-pod
	namespace: dev
	
	labels:
		app: myapp
		type: front-end
spec:
	containers:
	-	name: nginx-container
		image: nginx
		
kubectl create -f pod-defintion.yml

namespace-dev.yml

apiVersion: v1

kind: Namespace
metadata:
	name: dev



kubectl get namespaces

kubectl get ns

kubectl get pods --namespace=research

kubectl get pods -n=research

kubectl run redis --image=redis -n=finance

kubectl get pods --all-namespaces

kubectl get pods -A

kubectl config set-context $(kubectl config current-context) --namespace=dev


*******************************************************************************************************************************************************************************
IMPERATIVE Approach

Create Objects
kubectl run --image=nginx nginx

kubectl create deployment --image=nginx nginx

kubectl expose deployment ngninx 

Update Objects

kubectl edit deployment nginx

kubectl scale deployment nginx --replicas=5

kubectl set image deployment nginx nginx=nginx:1.1



kubectl create -f nginx.yml

kubectl replace -f nginx.yml

kubectl delete -f nginx.yml

DECLARATIVE Approach

kubectl apply -f nginx.yml


kubectl run nginx --image=nginx

kubectl run nginx --image=nginx --dry-client=client -o yaml

kubectl create deployment --image=nginx nginx

kubectl create deployment --image=nginx nginx --dry-client=client -o yaml

kubectl create deployment nginx --image=nginx replicas=4

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml

Service
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

kubectl create service cluterip redis --tcp=6379:6379 --dry-run=client -o yaml

NodePort
kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-client -o yaml 

kubectl run httpd --image=httpd:alpine --port=80 --expose=true

 
*******************************************************************************************************************************************************************************
Scheduling

apiVersion: v1
kind: Pod
metadata: 
	name: nginx
	labels:
		name: nginx
spec:
	containers: 
	-	name: nginx
		image: nginx
		ports: 
		- 	containerPort: 8080
	nodeName: node02
	
Pod-bind-definition.yml

apiVersion: v1
kind: Binding
metadata: 
	name: nginx
target: 
	apiVersion: v1
	kind: Node
	name: node02


kubectl get nodes

kubectl get pods --selector app=App1


apiVersion: apps/v1
kind: ReplicaSet
metadata: 
	name: simple-webapp
	labels:
		app: App1
		function : Front-end
	annotations:
		buildversion: 1.34
spec:
	replicas: 3
	selector: 
		matchLables: 
			app: App1
	template:
		metadata: 
			labels: 
				app: App1
				function: Front-end
		spec:
			containers:
			- 	name: simple-webapp
				image: simple-webapp
				
				
kubectl get pods --selector env=dev

kubectl get pods --selector env=dev | wc -l

kubectl get pods --selector env=dev --no-header | wc -l

kubectl get pods --selector bu=finance --no-header | wc -l

kubectl get pods --selector env=prod --no-headers 

kubectl get all --selector env=prod --no-headers | wc -l

kubectl get pods --selector env=prod,bu=finance,tier=frontend 

*******************************************************************************************************************************************************************************
Taint and Tolerations

kubectl taint nodes nodel app=blue:NoSchedule

pod-defintion.yml

apiVersion: v1
kind: Pod
metadata: 
	name: myapp-pod
spec:
	containers:
	-	name: nginx-container
		image: nginx
	tolerations: 
	-	key: "app"
		operator: "Equal"
		value: "blue"
		effect: "NoSchedule"
		
kubectl describe node kubemaster | grep Taint


kubectl get nodes

kubectl describe node node01

kubectl taint nodes node01 spray=mortein:NoSchedule

kubectl describe node node01

kubectl run mosquito --image-nginx

kubectl run bee --image=nginx --dry-client -o yaml > bee.yml

apiVersion: v1
kind: Pod
metadata: 
	creationTimestamp: null
	labels: 
		run: bee
	name: bee
spec:
		containers:
		- 	image: nginx
			name: bee
			resources: {}
		dnsPolicy: ClusterFirst
		restartPolicy: Always
		tolerations: 
		- 	key: "spray"
			operator: "Equal"
			value: "mortein"
			effect: "NoSchedule"
status: {}

kubectl create -f bee.yml

kubectl get pods -o wide

kubectl describe node controlplane 

#to remove the taint from the node
kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule-
node-role.kubernetes.io/control-plane:NoSchedule
		
		
*******************************************************************************************************************************************************************************
Node Selectors
pod-defintion.yml
apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
spec: 
	containers:
	-	name: data-processor
		image: data-processor
		
	nodeSelector:
		size: Large
		
kubectl label nodes node-1 size=Large

kubectl create -f pod-defintion.yml		

*******************************************************************************************************************************************************************************
Node Affinity
pod-defintion.yml

apiVersion: v1
kind: Pod

metadata:
	name: myapp-pod
spec:
	containers:
	-	name: data-processor
		image: data-processor
	affinity: 
		nodeAffinity: 
			requiredDuringSchedulingIgnoredDuringExecution:
				nodeSelectors:
				-	matchExpressions:
					-	key: size
						operator: In
						values: 
						-	Large
						-	Medium
						
						
						# does not considers Small. Ignores the values entered.
						key: size
						operator: NotIn
						values:
						-	Small	
						
						#checks if label for size exist or not. 
						key:	size
						operator: Exists

Node Affinity Types
	Available:
		requiredDuringSchedulingIgnoredDuringExecution
		prefferedDuringSchedulingIgnoredDuringExecution
	Planned For Future:
		requiredDuringSchedulingRequiredDuringExecution
		
kubectl describe node node01

kubectl label node node01 color=blue

kubectl create deployment blue --image=nginx --replicas=3

kubectl describe node node01 | grep Taints

# To edit existing nodes creted
kubectl edit deployment blue


shortcut
Cap V and then select all the lines. Press Shift + > button to align the indentation.

kubectl get pods -o wide

kubectl create deployment red --image=nginx --replicas=2 --dry-run=client -o yaml > red.yml

*******************************************************************************************************************************************************************************
Resource Requirements and Limit

pod-defintion.yml

apiVersion: v1
kind: Pod
metadata: 
	name: simple-webapp-color
	labels:
		name: simple-webapp-color
spec:
	containers:
	-	name: simple-webapp-color
		image: simple-webapp-color
		ports: 
			- containerPort: 8080
		resources:
			requests:
				memory: "1G"
				cpu: 1
			limits:
				memory: "2G"           // you can use 2G as GigaByte or 2Gi as GigiByte
				cpu: 2
				
1 CPU is equivalent to 1 AWS vCPU or 1 GCP Core or 1 Azure Core or 1 HyperThread

limit-range-cpu.yml

apiVersion: v1
kind: LimitRange
metadata: 
	name: cpu-resource-constraint
spec:
	limits: 
	-	default: 
			cpu: 500m
		defaultRequest: 
			cpu: 500m
		max: 
			cpu: "1"
		min:
			cpu: "1"
		type: Container
		
		
limit-range-memory.yml

apiVersion: v1
kind: LimitRange
metadata: 
	name: memory-resource-constraint
spec:
	limits:
	-	default:
			memory: 1G
		defaultRequest:
			memory: 1G
		max:
			memory: 1G
		min:
			memory: 500m
		type: Container

resource-quota.yml

apiVersion: v1
kind: ResourceQuota
metadata:
	name: my-resource-quota
spec:
	hard:
		requests.cpu: 4
		requests.memory: 4G
		limits.cpu: 10
		limits.memory: 10G
		

kubectl edit ctl pod <pod name>

kubectl delete pod webapp

kubectl create -f /tmp/kubectl-edit-ccvvrq.yml


kubectl get pod webapp -o yaml > new-pod.yml

vi new-pod.yml

kubectl delete pod webapp

kubectl create -f new-pod.yml

kubectl edit deployment my-deployment 

kubectl describe describe rabbit

kubectl delete pod rabbit
		
kubectl replace --force -f /tmp/kubectl-edit-3376288381.yaml                               --------------> rememeber these commands


*******************************************************************************************************************************************************************************
Daemon Sets
Daemon Sets example kub-proxy(worker node), weave.net (Networking)

daemon-set-definition.yml

apiVersion: apps/v1
kind: DaemonSet
metadata: 
	name: monitoring-daemon
spec:
	selector:
		matchLabels:
			app: monitoring-agent
	template:
		metadata:
			labels:
				app: monitoring-agent
		spec:
			containers:
			- 	name: monitoring-agent
				image: monitoring-agent
				
Kubectl create -f daemon-set-definition.yml

Kubegtl get daemonsets

Kubectl describe daemonsets monitoring-agent

kubectl get daemonsets -A

kubectl describe daemonsets kube-proxy -n kube-system      # need to mention the namespace

kubectl describe ds kube-flannel-ds -n kube-system

kubectl create deployment elasticsearch --image=registry.k8s.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > elastic-search.yml


apiVersion: apps/v1
kind: DaemonSet
metadata:
	labels:
		app: elasticsearch
	name: elasticsearch
	namespace: kube-system
spec:
	selector:
		matchLabels:
			app: elasticsearch
	template:
		metadata: 
			labels:
				app: elasticsearch
		spec: 
			containers:
			-	image: registry.k8s.io/fluentd-elasticsearch:1.20
				name: fluentd-elasticsearch
				
kubectl create -f fluent.yml

kubectl get daemonsets -n kube-system

*******************************************************************************************************************************************************************************

Static Pods

#### kubelet.service: path where the pod.yml file can be stored.
--pod-manifest-path= /etc/kubernetes/manifest

--config=kubeconfig.yml
staticPodPath = /etc/Kubernetes/manifest

docker ps

Static Pod is created by Kubelet

Daemonset is created by Kube-API Server.

Static Pod and DaemonSet are ignored by Kube-Scheduler

Static Pod deploys Control plane components
DaemonSets deploys Monitoring Agent, Logging Agents on nodes

kubectl get pod kube-apiserver-controlplane -n kube-system -o yaml

# address for the config file in Kubernetes
ls /var/lib/kubelet
cat /var/lib/kubelet/config.yml


cat /etc/kubernetes/manifests/etcd.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.16.199.12:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.16.199.12:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.16.199.12:2380
    - --initial-cluster=controlplane=https://192.16.199.12:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.16.199.12:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.16.199.12:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.10-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}

cat /etc/kubernetes/manifests/kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.16.199.12:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.16.199.12
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.16.199.12
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.16.199.12
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.16.199.12
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}

cat /etc/kubernetes/manifests/kube-controller-manager.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-controller-manager:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}

cat /etc/kubernetes/manifests/kube-scheduler.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    image: registry.k8s.io/kube-scheduler:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}   

kubectl run static-busybox --image=busybox --dry-run=client -o yaml --command -- sleep 1000 > static-busybox.yml

cp static-busybox.yml /etc/Kubernetes/manifests

kubectl get nodes -o wide

ssh 192.16.2.34
yes
cat /var/lib/kube-let

cd /etc/just-to-mess-with-you


rm greenbox.yml

*******************************************************************************************************************************************************************************

Multiple Schedulers


my-custom-scheduler.yml

apiVersion: v1
kind: Pod
metadata: 
	name: my-custom-scheduler
	namespace: kube-system
spec: 
	containers:
	- 	command:
		-	kube-scheduler
		- 	--address=127.0.0.1
		- 	--kubeconfig=/etc/kubernetes/scheduler.conf
		-	--config=/etc/kubernetes/my-scheduler-config.yml
		
		image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3
		name: kubescheduler
		
my-scheduler-config.yml

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
-	scheduleName: my-scheduler	
leaderElection:
	leadElect: true
	resourceNamespace: kube-system
	resourceName: lock-object-my-scheduler
	
pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
	name: nginx
spec: 
	containers:
	-	name: nginx
		name: nginx
	schedulerName: my-custom-scheduler
	
kubectl create -f pod-defintion.yml

kubectl get events -o wide

kubectl logs my-custom-scheduler --name-space=kube-system

kubectl describe pod kube-scheduler-controlplane -n kube-system

kubectl get serviceaccount -n kube-system and kubectl get clusterrolebinding

kubectl create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml -n kube-system

kubectl get configmap -n kube-system

apiVersion: v1
kind: Pod
metadata: 
	name: nginx
spec:
	schedulerName: my-scheduler
	containers:
	-	image: nginx
		name: nginx
		

*******************************************************************************************************************************************************************************

Scheduler Profiles

pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
	priorityClassName: high-priority
	containers:
	-	name: simple-webapp-color
		image: simple-webapp-color
		ports: 
		- containerPort: 8080
		nodeName: node02
		
		resources: 
			requests:
				memory: "1G"
				cpu: 10
				
Priority Class

apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
	name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."

Different stages for Scheduler: Scheduling Queue (PrioritySort plugin is attached to Extension point queueSort ) , Filtering (NodeResourcesFit, NodeName, NodeUnschedulable, TaintTolerations, NodePorts, NodeAffinity plugins are attached to Extension point filter) , Scoring (NodeResourcesFit , ImageLocality TaintTolerations Plugins are attached to Extension point Sort), Binding (DefaultBinder is attached to Extension point bind)

Extensions Points:
1) queueSort
2) preFilter
3) Filter
4) postFilter
5) preScore
6) score
7) reserve
8) permit
9) preBind
10) bind
11) postBind

scheduler-config.yaml
		
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- schedulerName: Default-scheduler

my-scheduler-config.yaml

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
-	schedulerName: my-scheduler


my-scheduler-2-config.yaml

apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
	- 	schedulerName: my-scheduler-2
		plugins: 
			score:
				disabled:
				- name: TaintToleration
				enabled: 
				- name: MyCustomPluginA
				- name: MyCustomPluginB
				
				
	- 	schedulerName: my-scheduler-3
		plugins:
			prescore:
				disabled:
				- name: '*'
			score:
				disabled:
				- name: '*'
		
	- 	schedulerName: my-scheduler-4
	
kubectl logs my-custom-scheduler --name-space=kube-system
	
 
https://github.com/kubernetes/enhancements

https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/

https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md


https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/


https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/


https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

*******************************************************************************************************************************************************************************
Monitoring Kubernetes
cAdvisor for Kublet send informations about pods.

Monitoring Solution
1) METRICS SERVER (in-memory solution, does not store historical data)
2) Prometheus
3) Elastic Stack
4) DATADOG
5) dynatrace

minikube addons enable metrics-server

git clone https://github.com/kubernetes-incubator/metrics-serve

kubectl create -f deploy/1.8+/

kubectl top node

kubectl top pod

kubectl create -f .

docker run kodekloud/event-simulator

docker run -d kodekloud/event-simulator      # logs will not be displayed on screen

docker logs -f docker-name  # display logs live trail

event-simulator.yaml

apiVersion: v1
kind: Pod
metadata: 
	name: event-simulator-pod
spec: 
	containers:
	-	name: event-simulator
		image: kodekloud/event-simulator
	- 	name: image-processor
		image: some-image-processor
		
kubectl create -f event-simulator.yaml

kubectl logs -f event-simulator-pod

kubectl logs - f event-simulator-pod event-simulator   # need to mention the name of the container if more than one containers are present.


*******************************************************************************************************************************************************************************
Application Lifecyle management

Rolling Updates and Rolling in Deployments, Configure Aplications , Scale Applications, Self-Healing Application 

kubectl apply -f deployment-definition.yml

kubectl set image deployment/myapp-deployment \nginx-container= nginx:1.7.1

kubectl rollout status deployment/myapp-development

kubectl rollout history deployment/myapp-deployment

#Recreate and Roplling Update 

kubectl get replicasets


kubectl create -f deployment-defintion.yml     								# To Create
kubectl get deployments 													# To Get
kubectl apply -f deployment-definition.yml 									# To Update
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1				# To Update
kubectl rollout status deployment/myapp-deployment							# To get Status
kubectl rollout history deployment/myapp-deployment							# To get Status
kubectl rollout undo deployment/myapp-deployment							# To Rollback


kubectl describe deploy frontend

kubectl edit deploy frontend

kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v2

docker run ubuntu

docker ps -a 

docker run ubuntu sleep 5

docker build -t ubuntu-sleeper

docker run ubuntu-sleeper

FROM ubuntu

CMD sleep 5


FROM ubuntu 

ENTRYPOINT ["sleep"]

docker run ubuntu-sleeper 10

FROM ubuntu

ENTRYPOINT ["sleep"]

CMD ["5"]

docker run ubuntu-sleeper 10

docker run --entrypoint sleep2.0 ubuntu-sleeper 10

docker run --name ubuntu-sleeper ubuntu-sleeper

pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
	name: ubuntu-sleeper-pod
spec: 
	containers:
	-	name: ubuntu-sleeper
		image: ubuntu-sleeper
		command: ["sleep2.0"]
		args: ["10"]
		
		
kubectl describe pod ubuntu-sleeper

kubectl edit pod ubuntu-sleeper-3

kubectl replace --force -f /tmp/kubectl-edit-3869459525.yaml

#DockerFile

FROM python:3.6-alpine 

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT  ["python" , "app.y"]

CMD ["--color" , "red"]
 


		

kubectl create -f pod-defintion.yml


FROM python:3.6-alpine

RUN pip install flask

COPY . /opt/

EXPOSE 8080

WORKDIR /opt

ENTRYPOINT ["python", "app.py"]

CMD ["--color", "red"]



apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["--color","green"]

kubectl run webapp-green --image=kodekloud/webapp-color --dry-run=client -o yaml

### Set environment variables

docker run -e APP_COLOR=pink simple-webapp-color

apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
spec:
	containers:
	-	name: simple-webapp-color
		image: simple-webapp-color
		ports: 
		- 	containerPort: 8080
		env:
		-	name: APP_COLOR
			value: pink
			
Types of Environments values
1) Plain Key Value
2) Config Map
3) Secrets


*******************************************************************************************************************************************************************************
kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-file=APP_MOD=prod

config-map.yml

apiVersion: v1
kind: ConfigMap
metadata: 
	name: app-config
data: 
	APP_COLOR: blue
	APP_MODE: prod
	
kubectl create -f config-map.yml

kubectl get configmaps

kubectl describe configmaps

pod-definition.yml

apiVersion: v1
kind: Pod
metadata: 
	name: simple-webapp-color
	labels:
		name: simple-webapp-color
spec: 
	containers:
	-	name: simple-webapp-color
		image: simple-webapp-color
		ports: 
			- containerPort: 8080
		envFrom:
			- configMapRef: 
				name: app-config
				
kubectl edit pod webapp-color

kubectl replace --force -f /tmp/kubectl-edit-2800178841.yaml

kubectl get configmap

kubectl describe cm db-config

kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2
 
kuectl create secret generic

kubectl create secret generic app-secret --from-literal=DB_Host=mysql

secret-data.yaml

apiVErsion: v1
kind:Secret
metadata: 
	name: app-secret
data: 
	DB_Host: mysql
	DB_User: root
	DB_Password: password
	
kubectle create -f secret-data.yaml

echo -n 'mysql' | base64
bX1zcWw=

kubectl get secrets

kubectl describe secrets

kubectl get secret app-secret -o yaml

echo -n 'bX1zcWw=' | base84 --decode


pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
	name: simple-webapp-color
	labels: 
		name: simple-webapp-color
spec:
	containers:
	-	name: simple-webapp-color
		image: simple-webapp-color
		ports: 
			-	containerPort: 8080
		envFrom:
			-	secretRef:
					name: app-secret
					
kubectl get secret

kubectl describe secret db-secret

kubectl describe pod webapp-pod

kubectl edit pod webapp-pod

kubectl replace --force -f /tmp/kubectl-edit-799322353.yaml

kubectl get my-secret -o yaml

apt-get install etcd-client  

kubectl get pods -n kube-system

ls /etc/kubernetes/pki/etcd


kubectl create secret generic my-secret --from-literal=key1=supersecret

kubectl get secret 

kubectl describe secret my-scret

kubectl get secret my-secret -o yaml

ETCDCTL_API=3 etcdctl \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt   \
   --cert=/etc/kubernetes/pki/etcd/server.crt \
   --key=/etc/kubernetes/pki/etcd/server.key  \
   get /registry/secrets/default/secret1 | hexdump -C
   
ps -aux | grep kube-api

cat /etc/kubernetes/manifests/kube-apiserver.yaml


#
# CAUTION: this is an example configuration.
#          Do not use this for your own cluster!
#
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
      - configmaps
      - pandas.awesome.bears.example # a custom resource API
    providers:
      # This configuration does not provide data confidentiality. The first
      # configured provider is specifying the "identity" mechanism, which
      # stores resources as plain text.
      #
      - identity: {} # plain text, in other words NO encryption
      - aesgcm:
          keys:
            - name: key1
              secret: c2VjcmV0IGlzIHNlY3VyZQ==
            - name: key2
              secret: dGhpcyBpcyBwYXNzd29yZA==
      - aescbc:
          keys:
            - name: key1
              secret: c2VjcmV0IGlzIHNlY3VyZQ==
            - name: key2
              secret: dGhpcyBpcyBwYXNzd29yZA==
      - secretbox:
          keys:
            - name: key1
              secret: YWJjZGVmZ2hpamtsbW5vcHFyc3R1dnd4eXoxMjM0NTY=
  - resources:
      - events
    providers:
      - identity: {} # do not encrypt Events even though *.* is specified below
  - resources:
      - '*.apps' # wildcard match requires Kubernetes 1.27 or later
    providers:
      - aescbc:
          keys:
          - name: key2
            secret: c2VjcmV0IGlzIHNlY3VyZSwgb3IgaXMgaXQ/Cg==
  - resources:
      - '*.*' # wildcard match requires Kubernetes 1.27 or later
    providers:
      - aescbc:
          keys:
          - name: key3
            secret: c2VjcmV0IGlzIHNlY3VyZSwgSSB0aGluaw==
			
crictl pods

ps aux | grep kube-api



*******************************************************************************************************************************************************************************

Multi Container pod

pod-definition.yml

apiVersion: v1
kind: Pod
metadata: 
	name: simple-webapp
	labels:
		name: simple-webapp
spec:
	containers:
	-	name: simple-webapp
		image: simple-webapp
		ports:
			-	containerPort: 8080
	- 	name: log-agent
		image: log-agent
		
		
		
yellow.yml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    resources: {}
    command: ["sleep", "1000"]  
  - image: redis
    name: gold
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl create -f yellow.yml

kubectl get pod -n elastic-stack

kubectl -n elastic-stack logs kibana

kubectl logs app -n elastic-stack

kubectl -n elastic-stack exec -it app -- cat /log/app.log

kubectl edit pod app -n elastic-stack 

kubectl replace --force -f /tmp/kubectl-edit-1263888539.yaml

InitContainers

apiVersion: v1
kind: Pod
metadata: 
	name: myapp-pod
	labels:
		name: myapp
spec:
	containers: 
	-	name: myapp-container
		image: busybox:1.28
		command: ['sh','-c', 'echo Tha app is running! & sleep 3600']
	initContainers:
	- 	name: init-myservice
		image: busybox:1.28
		commmand: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice sleep 2; done;']
	-	name: init-mydb
		image: busybox:1.28
		command: ['sh','-c','until nslookup myservice; do echo waiting for myservice sleep 2; done;']
		
		
kubectl logs orange -c init-service


*******************************************************************************************************************************************************************************
Cluster Maintainance

kubectl drain node-1

kubectl uncordon node-1

kubectl cordon node-2  

kubectl get deploy       # get applications hosted on the cluster

kubectl drain node01 --ignore-daemonsets

kubectl get pods -o wide

kubectl drain node01 --ignore-daemonsets --force 			##deleting Pods that declare no controller 

kubectl cordon node01

kubeadm upgrade plan
Components API server, Controller Manager, Scheduler and Kube Proxy are upgrade upgrade autmatically
CoreDNS and ETCD not part of COre Kubernetes

apt-get upgrade -y kubeadm=1.12.0-00

kubeadm upgrade apply v1.12.0

apt-get upgrade -y kubelet-1.12.0-00

systemctl restart kubelet

kubectl drain node-1

apt-get upgrade -y kubeadm=1.12.0-00

apt-get upgrade -y kubelet=1.12.0-00

kubeadm upgrade node config --kubelet-version v1.12.0

systemctl restart kubelet

kubectl uncordon node-1 

Manually upgrady Kubectl component

#update Kubeadm first 
cat /etc/*release*

ssh node01

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

sudo apt-get update

sudo apt update
sudo apt-cache madison kubeadm

#Upgrade kubeadm: 1.29.3-1.1

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

#verify if the version is upgraded
kubeadm version

sudo kubeadm upgrade plan

sudo kubeadm upgrade apply v1.29.3

# Upgrade the kubelet and kubectl

kubectl drain control-plane --ignore-daemonsets

sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl uncordon control-plane

kubectl describe node | grep Taints

kubectl get deploy

kubectl get pods -o wide

kubectl drain control-plane --ignore-daemonsets
1.28.8-1.1
1.28.8-1.1

apt-get install -y --allow-change-held-packages kubeadm-1.29.0-00

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.28.8-1.1' && \
sudo apt-mark hold kubeadm

sudo kubeadm upgrade apply v1.28.8

sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.28.8-1.1' kubectl='1.28.8-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet


apt-get upgrade -y kubeadm=1.28.8-1.1

apt-get upgrade -y kubelet=1.28.8-1.1

___________--------------------------------------------------------------------------------------
cat /etc/*release*

sudo apt update
sudo apt-cache madison kubeadm

apt --version

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.28.8-1.1' && \
sudo apt-mark hold kubeadm

kubeadm version

sudo kubeadm upgrade plan

sudo kubeadm upgrade apply v1.29.0

sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

kubectl get nodes


kubectl uncordon controlplane

kubectl drain node01 --ignore-daemonsets

ssh node01

sudo apt-get update

sudo apt update
sudo apt-cache madison kubeadm

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.29.0-1.1' kubectl='1.29.0-1.1' && \
sudo apt-mark hold kubelet kubectl

exit

kubectl get nodes
kubectl uncordon node01


cat /etc/*release*

sudo kubeadm upgrade apply v1.29.0

kubectl drain controlplane --ignore-daemonsets

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.29.0-1.1' && \
sudo apt-mark hold kubeadm

*******************************************************************************************************************************************************************************
BackUp Restore

kubectl create namespace new-namespace

kubectl create secret

kubectl create configmap

pod-definition.yml
apiVersion: v1
kind: Pod

metadata: 
	name: myapp-pod
	labels:
		app: myapp
		type: front-end
spec: 
	containers:
	- 	name: nginx-container
		image: nginx
		
kubectl apply -f pod-definition.yml

kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml

VelERO(earliee called Ark ) can be be used for taking backups for Kubernetes  

ETCDCTL_API=3 etcdctl \ snapshot save snapshot.db

ETCDCTL_API=3 etcdctl \ snapshot status snapshot.db

#restore using ETCD backup
service kube-apiserver stop

ETCDCTL_API=3 etcdctl \ snapshot restore snapshot.db  \ --data-dir /var/lib/etcd-from-backup

# configure the etcd.service 
systemctl daemon-reload

service etcd restart

service kube-apiserver start

ETCDCTL_API=3 etcdctl snapshot save snapshot.db --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd-server.crt --key=/etc/etcd/etcd-server.key

kubectl get pods -n kube-system

kubectl describe pod etcd-controlplane -n kube-system

ls /etc/kubernetes/manifests

#File etcd.yaml

apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.21.14.6:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.21.14.6:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.21.14.6:2380
    - --initial-cluster=controlplane=https://192.21.14.6:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.21.14.6:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.21.14.6:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.10-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {} 

ls /var/lib/etcd/

ETCDCTL_API=3 etcdctl snapshot

EXPORT ETCDCTL_API=3

 etcdctl snapshot save --endpoints=127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key /opt/snapshot-pre-boot.db 
 
 kubectl get svc
 
 etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup /opt/snapshot-pre-boot.db
 
 vi /etc/kubernetes/manifests/etcd.yaml

/etc/kubernetes/manifests/etcd.yaml

apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.21.14.6:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.21.14.6:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.21.14.6:2380
    - --initial-cluster=controlplane=https://192.21.14.6:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.21.14.6:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.21.14.6:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.10-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
status: {}

kubectl get pods -n kube-system

kubectl get pods -n kube-system --watch

kubectl logs -n kube-system etcd-controlplane

kubectl describe pod etcd-conrolplane -n kube-system

#Shows all the clusters that are configured
kubectl config view

kubectl config use-context cluster1
kubectl get nodes

ssh cluster2-controlplane

kubectl describe pod kube-apiserver-cluster1-controlplane -n kube-system

ETCDCTL_API=3 etcdctl snapshot save /opt/cluster1.db

ps -ef | grep -i etcd

ETCDCTL_API=3  etcdctl --endpoints=192.9.45.18:2379 --cacert=/etc/kubernetes/pki/etcd/ca.pem  --cert=/etc/kubernetes/pki/etcd/etcd.pem  --key=/etc/kubernetes/pki/etcd/etcd-key.pem member list

#copy file from one node to other
scp cluster1-controlplane:/opt/cluster1.db /opt/cluster1.db 

ls -la
chown -R etcd:etcd etcd-data-new 


*******************************************************************************************************************************************************************************
### Kubernetes Security
	1) kubernetes Security Primitives
	2) Secure Persistent Key Value Store
	3) Authentication
	4) Authorization
	5) Security Contexts
	6) TLS Certificates for Cluster Components
	7) Images Securely
	8) Network Policies
	
Kubectl create user user1

kubectl list users

kubectl create serviceaccount sa1

kubectl get serviceaccount

# Auth Mechanism
1) Static Password File    # --basic-auth-file=user-details.csv
2) Static Token File 		# --token-auth-file=user-token-details.csv
3) Certificates
4) Identity Services 

curl -v k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer Ksayug78e398ydskjvmb34r87"

kube-apiserver.yaml

apiVersion: v1
kind: Pod
metadata: 
	creationTimestamp: null
	name: kube-apiserver
	namespace: kube-system
spec:
	containers:
	-	command:
		- 	kube-apiserver
		-	--authorization-mode=Node,RBAC
		-	--advertise-address=172.17.0.107
		-	--allow-priveleged=true
		-	--enable-admission-plugins=NodeRestriction
		-	--enable-bootstrap-token-auth=true
		-	--basic-auth-file-user-details.csv
		image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
		name: kube-apiserver	

/etc/kubernetes/manifests/kube-apiserver.yaml

    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-apiserver
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-apiserver
          <content-hidden>
        image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
        name: kube-apiserver
        volumeMounts:
        - mountPath: /tmp/users
          name: usr-details
          readOnly: true
      volumes:
      - hostPath:
          path: /tmp/users
          type: DirectoryOrCreate
        name: usr-details

    apiVersion: v1
    kind: Pod
    metadata:
      creationTimestamp: null
      name: kube-apiserver
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-apiserver
        - --authorization-mode=Node,RBAC
          <content-hidden>
        - --basic-auth-file=/tmp/users/user-details.csv
		
#Create the necessary roles and role bindings for these users:


    ---
    kind: Role
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      namespace: default
      name: pod-reader
    rules:
    - apiGroups: [""] # "" indicates the core API group
      resources: ["pods"]
      verbs: ["get", "watch", "list"]
     
    ---
    # This role binding allows "jane" to read pods in the "default" namespace.
    kind: RoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: read-pods
      namespace: default
    subjects:
    - kind: User
      name: user1 # Name is case sensitive
      apiGroup: rbac.authorization.k8s.io
    roleRef:
      kind: Role #this must be Role or ClusterRole
      name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
      apiGroup: rbac.authorization.k8s.io
	
#Once created, you may authenticate into the kube-api server using the users credentials

curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123" 

#Setup basic authentication on Kubernetes (Deprecated in 1.19)

ssh-keychain                	# generates private key and public key

ssh -i id_rsa user1@server1

cat ~/.ssh/authorized_keys

openssl genrsa -out my-bank.key 1024   			# to generate private key

openssl rsa -in my-bank.key -pubout > mybank.pem

# Certificate Authority List
1) Symantec
2) Comodo
3) Global Sign
4) DigiCert

openssl req -new -key my-bank.key -out my-bank.csr -subj "/C=US/ST =CA/O=MyOrg, Inc./CN=mydomain.com"

#### public key  *.crt *.pem
#### private key *.key *-key.pem



### Certifcates Types
1) Root Certificates
2) Client Certificates
3) Server Certificates

## Tools to generate certificates EASYRSA, OPENSSL and CFSSL.

#Generate Certificate Keys
openssl genrsa -out ca.key 2048

#Certifcate Signing request having no signature
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr

#Sign Certificates
openssl x509 -req -in ca.csr -signkey -out ca.crt 


#Create Admin User Certificates
#CreateKey
openssl genrsa -out admin.key 2048

#Create Certificate having no signature
openssl req -new -key admin.key -subj "\CN=kube-admin/O=system:masters" -out admin.csr

# Sign Certifcates
openssl x509 -req -in amdin.csr -CA ca.crt -CAkey ca.key -out admin.crt

curl https://kube-apiserver:6443/api/v1/pods --key admin.key --cert admin.cert --cacert ca.crt

kube-config.yaml

apiVersion: v1
clusters: 
-	cluster: 
		certificate-authority: ca.crt
		server: https://kube-apiserver:6443
	name: kubernetes
kind: Config
users: 
-	name: kubernetes-admin
	user:
		client-certificate: admin.cert
		client-key: admin.key
		
		
#openssl.cnf
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[v3_req]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation,
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 10.96.0.1
IP.2 = 172.17.0.87

openssl req -new -key apiserver -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.conf

openssl X509 -req -in apiserver -CA ca.crt -CAKey -out apiserver.crt


## kubelet-config.yaml

kind : KubeletConfiguration
apiVersion : kubelet.config.k8s.io/v1beta1
authentication: 
	x509: 
		clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization: 
	node: Webhook
clusterDomain: "cluster.local"
clusterDNS: 
	- "10.32.0.10"
podCIDR: "${POD_CTDR}"
resolveConf: "/run/systemd/resolve/resolve.conf"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/kubelet-node01.crt"
tlsPrivateKeyFile: "/var/lib/kubelet/kubelet-node01.key"

## To deocde the certificate
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

#kube-apiserver.yaml

spec:
	containers:
	- 	command:
		-	kube-apiserver
		-	--authorization-mode=Node, RBAC
		-	--advertise-address=172.17.0.32
		-	--allow-privileged=true
		-	--client-ca-file=/etc/kubernetes/pki/ca.crt
		-	--disable-admission-plugins-PersistentVolumeLabel
		-	--enable-admission-plugins-NodeRestriction
		-	--enable-bootstrap-token-auth=true
		-	--etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
		-	--etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt 
		-	--etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
		-	--etcd-servers=https://127.0.0.1:2379
		-	--insecure-port=0
		-	--kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt 
		-	--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
		-	--kubelet-preferred-address-types-InternalIP, ExternalIP, Hostname
		-	--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
		-	--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
		-	--secure-port=6443
		-	--service-account-key-file=/etc/kubernetes/pki/sa.pub
		-	--service-cluster-ip-range-10.96.0.0/12
		-	--tls-cert-file=/etc/kubernetes/pki/apiserver.crt
		-	--tls-private-key-file=/etc/kubernetes/pki/apiserver.key
		
#Inspect Service Logs
journalctl -u etcd.service -l

kubectl logs etcd-master

#List all dockers
docker ps -a

docker logs 87fc

cat /etc/kubernetes/manifests/kube-apiserver.yaml

apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.13.200.9:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.13.200.9
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.13.200.9
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.13.200.9
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.13.200.9
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}


cat /etc/kubernetes/manifests/etcd.yaml

apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.13.200.9:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.13.200.9:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.13.200.9:2380
    - --initial-cluster=controlplane=https://192.13.200.9:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.13.200.9:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.13.200.9:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.10-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}

openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -nout

#sample copy of certificate
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 3080349426628546566 (0x2abf997bae45d006)
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN = kubernetes
        Validity
            Not Before: Apr  7 10:08:34 2024 GMT
            Not After : Apr  7 10:13:34 2025 GMT
        Subject: CN = kube-apiserver
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:a5:a6:33:bd:ea:e9:bf:c0:76:a6:fc:8b:c7:ff:
                    09:57:03:51:1f:68:09:75:0a:85:03:08:bc:0f:25:
                    dc:58:10:3c:d0:9f:cc:2d:ca:04:91:1c:89:15:0c:
                    f7:0d:1d:d5:13:9d:f2:84:eb:c3:db:85:8f:1b:6b:
                    8c:e5:73:af:2d:85:ae:8b:83:5c:97:8c:5d:79:bb:
                    84:0b:3a:8b:91:57:cb:ff:2f:b9:ba:eb:22:e4:01:
                    b0:52:03:11:1b:7c:93:58:59:62:5d:23:5b:df:d1:
                    e5:05:8c:42:3c:fb:d5:93:29:37:dd:da:27:a3:37:
                    0e:9e:8d:d3:f7:42:ff:e3:9a:8e:be:1d:7a:23:3f:
                    4c:b1:dc:eb:cb:4d:1a:67:46:25:ae:15:4c:8e:d0:
                    32:8a:8a:ab:6d:1a:3a:52:8c:c9:1b:08:b6:da:e7:
                    13:38:ca:4d:1e:4e:ae:59:f5:f0:f7:ae:b2:b9:52:
                    42:0e:33:df:9b:4c:4f:0c:1a:0f:53:70:9a:aa:f9:
                    62:e1:3e:9b:12:12:42:03:0b:25:3c:86:c2:a0:1f:
                    bc:e8:46:9f:ff:bb:73:92:8f:b9:4f:28:4a:92:3f:
                    1f:62:05:0c:b3:57:6e:01:d0:70:a2:f8:6f:06:05:
                    f7:17:5f:26:49:1f:fe:d8:7a:d6:8e:83:fb:d6:95:
                    be:ef
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Authority Key Identifier: 
                F0:55:D8:C0:E4:50:E3:8B:9B:E0:73:D3:0F:1B:71:AC:FE:53:79:9F
            X509v3 Subject Alternative Name: 
                DNS:controlplane, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:192.13.200.9
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        b1:6f:01:5b:b2:ba:3e:77:67:6e:1c:87:fb:4b:24:f3:f0:37:
        60:51:45:82:9d:36:f2:dd:4f:73:c0:d7:9d:1a:5a:7d:4c:72:
        6a:e7:f8:5c:45:d8:e0:6d:09:4f:5e:9b:24:04:d1:ac:07:b4:
        e9:60:1c:1b:aa:f3:51:b3:68:6f:e1:4a:62:14:4b:3f:21:bd:
        02:e2:f9:4e:3f:8d:b6:a3:d2:94:75:d1:65:4a:f3:16:01:4d:
        7f:ea:91:9e:cc:3e:6b:3e:18:54:cd:ee:38:9a:bf:9d:83:8e:
        b6:70:5a:c7:8d:d8:f1:0e:70:30:fa:57:2a:4f:0d:75:54:2c:
        6f:86:dc:4f:16:8c:da:98:4f:85:78:59:10:58:a3:9c:3c:83:
        91:5c:40:d2:16:03:1e:94:e7:f9:4e:10:8c:e9:28:40:69:35:
        ca:6b:bd:b3:8c:48:5d:84:75:25:81:a9:42:a2:99:4a:5d:e0:
        00:3e:cd:ac:67:b2:df:c9:6b:e2:05:4b:a6:1f:c6:5e:36:1d:
        1d:75:9f:a9:76:96:22:a6:36:1f:16:35:63:f1:cd:ce:9f:ec:
        63:42:fb:d1:37:f1:f2:a4:ec:69:26:f6:14:a3:3d:a6:22:5c:
        e1:60:fd:92:af:68:f6:ef:4f:26:47:57:52:6a:a0:0e:be:5b:
        fa:8f:ba:e8
-----BEGIN CERTIFICATE-----
MIIDjDCCAnSgAwIBAgIIKr+Ze65F0AYwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
AxMKa3ViZXJuZXRlczAeFw0yNDA0MDcxMDA4MzRaFw0yNTA0MDcxMDEzMzRaMBkx
FzAVBgNVBAMTDmt1YmUtYXBpc2VydmVyMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A
MIIBCgKCAQEApaYzverpv8B2pvyLx/8JVwNRH2gJdQqFAwi8DyXcWBA80J/MLcoE
kRyJFQz3DR3VE53yhOvD24WPG2uM5XOvLYWui4Ncl4xdebuECzqLkVfL/y+5uusi
5AGwUgMRG3yTWFliXSNb39HlBYxCPPvVkyk33donozcOno3T90L/45qOvh16Iz9M
sdzry00aZ0YlrhVMjtAyioqrbRo6UozJGwi22ucTOMpNHk6uWfXw966yuVJCDjPf
m0xPDBoPU3Caqvli4T6bEhJCAwslPIbCoB+86Eaf/7tzko+5TyhKkj8fYgUMs1du
AdBwovhvBgX3F18mSR/+2HrWjoP71pW+7wIDAQABo4HbMIHYMA4GA1UdDwEB/wQE
AwIFoDATBgNVHSUEDDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB8GA1UdIwQY
MBaAFPBV2MDkUOOLm+Bz0w8bcaz+U3mfMIGBBgNVHREEejB4ggxjb250cm9scGxh
bmWCCmt1YmVybmV0ZXOCEmt1YmVybmV0ZXMuZGVmYXVsdIIWa3ViZXJuZXRlcy5k
ZWZhdWx0LnN2Y4Ika3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2Fs
hwQKYAABhwTADcgJMA0GCSqGSIb3DQEBCwUAA4IBAQCxbwFbsro+d2duHIf7SyTz
8DdgUUWCnTby3U9zwNedGlp9THJq5/hcRdjgbQlPXpskBNGsB7TpYBwbqvNRs2hv
4UpiFEs/Ib0C4vlOP422o9KUddFlSvMWAU1/6pGezD5rPhhUze44mr+dg462cFrH
jdjxDnAw+lcqTw11VCxvhtxPFozamE+FeFkQWKOcPIORXEDSFgMelOf5ThCM6ShA
aTXKa72zjEhdhHUlgalCoplKXeAAPs2sZ7LfyWviBUumH8ZeNh0ddZ+pdpYipjYf
FjVj8c3On+xjQvvRN/HypOxpJvYUoz2mIlzhYP2Sr2j2708mR1dSaqAOvlv6j7ro
-----END CERTIFICATE-----

openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -nout

#sample copy of certificate

Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 5292382433316753789 (0x497251c8695cd17d)
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN = etcd-ca
        Validity
            Not Before: Apr  7 10:08:35 2024 GMT
            Not After : Apr  7 10:13:35 2025 GMT
        Subject: CN = controlplane
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:da:94:4a:b6:39:e8:72:92:14:71:dc:54:3d:93:
                    9e:9d:ea:c8:d9:f1:ed:a3:21:a0:f3:22:c4:70:da:
                    1c:c1:29:49:a8:c4:0e:14:a6:53:fc:17:fc:ca:c7:
                    da:2a:36:1a:6f:20:2d:04:b7:6a:80:26:53:b7:4c:
                    ca:03:fe:9a:52:11:b4:3b:1d:4c:d7:89:94:8b:c7:
                    93:91:77:1c:05:eb:d6:af:1f:a4:b6:e9:78:00:25:
                    3d:17:52:74:5b:04:af:79:68:f3:1d:c9:85:d7:bb:
                    9f:26:b9:f5:bf:af:7d:30:9a:81:fb:0e:5d:0e:c7:
                    56:ad:6e:e3:75:4b:25:f5:04:5a:1e:2b:59:ac:ac:
                    fd:6e:b9:44:ae:da:06:7f:04:78:d8:85:69:97:13:
                    9c:c3:59:53:d9:e7:45:7c:7d:37:55:a7:ce:b3:14:
                    5e:90:5e:c4:14:5a:b1:2f:7e:76:72:1d:64:17:ef:
                    ba:39:80:8a:28:7e:a4:e4:b0:4f:4b:ed:78:3a:49:
                    0c:18:f4:d0:e3:7a:85:53:17:65:6e:22:da:7b:b5:
                    1d:63:e6:55:2e:46:18:3c:8e:ba:01:7c:f8:d6:a7:
                    8a:75:4a:49:81:3f:72:06:25:ac:23:4b:d2:36:07:
                    91:8e:07:ec:f1:6d:07:80:15:64:5a:91:a1:d6:41:
                    55:d7
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment
            X509v3 Extended Key Usage: 
                TLS Web Server Authentication, TLS Web Client Authentication
            X509v3 Basic Constraints: critical
                CA:FALSE
            X509v3 Authority Key Identifier: 
                E3:59:CD:99:97:5A:E3:76:72:08:16:3A:A0:E3:CB:4E:E1:5D:17:7F
            X509v3 Subject Alternative Name: 
                DNS:controlplane, DNS:localhost, IP Address:192.13.200.9, IP Address:127.0.0.1, IP Address:0:0:0:0:0:0:0:1
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        2e:27:6b:d3:d0:47:e1:31:01:61:c8:34:38:c4:56:8c:3a:fd:
        9a:1a:50:a1:97:b4:f2:16:db:4b:8e:27:34:b4:bf:6d:c5:7b:
        83:58:0b:b3:73:1a:a3:0e:4b:10:8b:da:b9:b0:3a:3b:2f:26:
        de:b1:1f:ac:4c:18:d3:a3:18:d8:6c:13:23:de:38:81:df:95:
        b3:f5:15:f1:51:75:13:c0:86:36:20:a1:f1:4c:bd:7e:6d:ee:
        d7:5c:8c:99:f5:b3:d8:2e:ba:58:9b:d6:51:5e:78:24:a4:8d:
        97:af:46:59:c6:4f:0d:52:7e:dc:c0:3a:2a:c4:14:59:8d:57:
        b8:22:a9:83:4c:3c:89:59:8e:e0:6a:82:4a:59:a2:72:bc:e8:
        2a:32:e3:27:93:96:2c:66:31:58:94:a1:e8:14:80:2f:7f:57:
        10:bf:e1:9c:65:a8:7e:75:25:49:69:24:f0:6e:16:3a:b2:c2:
        87:59:21:76:c1:1d:0a:e1:db:82:00:1c:86:b1:5f:77:64:18:
        dc:9f:d7:95:49:41:78:ee:e3:f9:3c:b0:e1:6d:3a:4a:84:32:
        33:61:45:79:3f:a7:a1:33:24:45:23:6c:b4:73:37:95:c4:c4:
        08:79:9b:2c:84:68:7c:5e:0e:e0:79:fc:44:9d:f4:ec:67:a8:
        32:f3:49:81
-----BEGIN CERTIFICATE-----
MIIDTzCCAjegAwIBAgIISXJRyGlc0X0wDQYJKoZIhvcNAQELBQAwEjEQMA4GA1UE
AxMHZXRjZC1jYTAeFw0yNDA0MDcxMDA4MzVaFw0yNTA0MDcxMDEzMzVaMBcxFTAT
BgNVBAMTDGNvbnRyb2xwbGFuZTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoC
ggEBANqUSrY56HKSFHHcVD2Tnp3qyNnx7aMhoPMixHDaHMEpSajEDhSmU/wX/MrH
2io2Gm8gLQS3aoAmU7dMygP+mlIRtDsdTNeJlIvHk5F3HAXr1q8fpLbpeAAlPRdS
dFsEr3lo8x3Jhde7nya59b+vfTCagfsOXQ7HVq1u43VLJfUEWh4rWays/W65RK7a
Bn8EeNiFaZcTnMNZU9nnRXx9N1WnzrMUXpBexBRasS9+dnIdZBfvujmAiih+pOSw
T0vteDpJDBj00ON6hVMXZW4i2nu1HWPmVS5GGDyOugF8+NaninVKSYE/cgYlrCNL
0jYHkY4H7PFtB4AVZFqRodZBVdcCAwEAAaOBozCBoDAOBgNVHQ8BAf8EBAMCBaAw
HQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHwYD
VR0jBBgwFoAU41nNmZda43ZyCBY6oOPLTuFdF38wQAYDVR0RBDkwN4IMY29udHJv
bHBsYW5lgglsb2NhbGhvc3SHBMANyAmHBH8AAAGHEAAAAAAAAAAAAAAAAAAAAAEw
DQYJKoZIhvcNAQELBQADggEBAC4na9PQR+ExAWHINDjEVow6/ZoaUKGXtPIW20uO
JzS0v23Fe4NYC7NzGqMOSxCL2rmwOjsvJt6xH6xMGNOjGNhsEyPeOIHflbP1FfFR
dRPAhjYgofFMvX5t7tdcjJn1s9guulib1lFeeCSkjZevRlnGTw1SftzAOirEFFmN
V7giqYNMPIlZjuBqgkpZonK86Coy4yeTlixmMViUoegUgC9/VxC/4ZxlqH51JUlp
JPBuFjqywodZIXbBHQrh24IAHIaxX3dkGNyf15VJQXju4/k8sOFtOkqEMjNhRXk/
p6EzJEUjbLRzN5XExAh5myyEaHxeDuB5/ESd9OxnqDLzSYE=
-----END CERTIFICATE-----


openssl x509 -in /etc/kubernetes/pki/ca.crt -text -nout

#sample copy of certificate

Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 550467664239739122 (0x7a3a76c935940f2)
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN = kubernetes
        Validity
            Not Before: Apr  7 10:08:34 2024 GMT
            Not After : Apr  5 10:13:34 2034 GMT
        Subject: CN = kubernetes
        Subject Public Key Info:
            Public Key Algorithm: rsaEncryption
                Public-Key: (2048 bit)
                Modulus:
                    00:bf:20:8b:ca:a1:10:6c:fb:91:24:96:ab:25:b4:
                    9a:10:00:8e:f8:f9:a5:c1:91:f6:a2:8b:93:0b:9c:
                    7a:64:6d:3e:81:49:dd:d1:ee:83:1a:1c:0c:70:c9:
                    53:a3:27:9e:e8:a7:65:f0:e2:6a:cf:8e:e1:ad:04:
                    45:43:13:a2:37:02:92:4f:3d:ef:a5:c3:91:50:d7:
                    6b:e4:ed:18:d1:5e:ab:d0:9f:12:a0:3a:64:e2:c9:
                    bd:12:4b:77:25:6c:0b:3f:b6:56:b5:88:46:2d:52:
                    bd:91:b4:9c:5f:2d:cd:a5:55:70:07:de:f6:87:a6:
                    54:59:70:19:f1:68:91:59:1e:94:17:04:7d:73:bb:
                    d0:79:e9:0c:05:ba:15:1a:95:aa:bd:ce:75:8a:46:
                    fc:5f:e1:b8:dc:85:bb:57:fe:4c:c5:06:b2:48:ac:
                    b7:cc:8d:70:45:f1:9c:77:d5:ff:bf:52:bd:7f:71:
                    2b:26:e3:6b:da:a4:62:8a:70:67:ed:83:0c:68:e8:
                    2e:cd:46:98:99:79:f0:61:dc:be:70:8b:96:a8:8e:
                    4e:e7:dd:1b:80:5f:1f:f0:f5:4b:41:3c:93:09:10:
                    f0:84:17:5f:b8:cd:ec:f2:93:d2:73:88:b3:6f:42:
                    4f:93:e2:56:c6:44:48:a6:e3:69:41:cb:77:a2:52:
                    57:43
                Exponent: 65537 (0x10001)
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature, Key Encipherment, Certificate Sign
            X509v3 Basic Constraints: critical
                CA:TRUE
            X509v3 Subject Key Identifier: 
                F0:55:D8:C0:E4:50:E3:8B:9B:E0:73:D3:0F:1B:71:AC:FE:53:79:9F
            X509v3 Subject Alternative Name: 
                DNS:kubernetes
    Signature Algorithm: sha256WithRSAEncryption
    Signature Value:
        76:0a:58:ee:75:80:34:0b:5e:92:7c:5d:36:db:54:61:13:66:
        69:de:86:5e:ac:0e:a9:93:e1:ce:80:49:e3:b0:2c:bf:5b:34:
        9a:65:23:17:5b:52:30:eb:15:01:13:ec:50:ee:09:ab:5f:ed:
        2e:cb:e3:11:1c:8b:f4:ad:da:9a:b0:58:a7:59:4c:38:e2:c8:
        e0:6c:9b:0a:35:f7:19:44:3d:93:00:81:05:ce:65:ec:ab:0b:
        86:68:ae:e5:37:cc:b1:a7:ff:6a:64:8b:16:5d:4f:98:dd:31:
        da:88:03:6e:96:62:82:3b:60:27:5c:c9:04:79:7d:93:0a:df:
        4f:0e:62:a8:ea:98:57:b6:05:eb:53:62:c8:0f:2f:8d:de:51:
        b0:91:fc:4a:18:52:56:d0:5d:f1:e0:39:09:e8:7a:ea:c3:76:
        aa:3d:ab:5a:e2:21:4c:25:d8:b2:a6:dc:8b:a2:db:e4:c4:8a:
        3b:18:2a:10:e4:e4:a1:e3:26:78:46:19:2b:f6:cb:5c:7c:66:
        7e:cf:d3:a9:ee:35:4b:64:bd:9d:b8:ed:08:16:24:ec:7d:db:
        3a:d4:7b:b0:ad:08:e8:dd:1a:3d:b7:2b:0f:3a:a8:e1:c6:1e:
        eb:ce:67:6b:80:77:4d:ba:66:84:08:b2:42:41:08:eb:a2:cc:
        9d:d9:48:fa
-----BEGIN CERTIFICATE-----
MIIDBTCCAe2gAwIBAgIIB6OnbJNZQPIwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE
AxMKa3ViZXJuZXRlczAeFw0yNDA0MDcxMDA4MzRaFw0zNDA0MDUxMDEzMzRaMBUx
EzARBgNVBAMTCmt1YmVybmV0ZXMwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
AoIBAQC/IIvKoRBs+5EklqsltJoQAI74+aXBkfaii5MLnHpkbT6BSd3R7oMaHAxw
yVOjJ57op2Xw4mrPjuGtBEVDE6I3ApJPPe+lw5FQ12vk7RjRXqvQnxKgOmTiyb0S
S3clbAs/tla1iEYtUr2RtJxfLc2lVXAH3vaHplRZcBnxaJFZHpQXBH1zu9B56QwF
uhUalaq9znWKRvxf4bjchbtX/kzFBrJIrLfMjXBF8Zx31f+/Ur1/cSsm42vapGKK
cGftgwxo6C7NRpiZefBh3L5wi5aojk7n3RuAXx/w9UtBPJMJEPCEF1+4zezyk9Jz
iLNvQk+T4lbGREim42lBy3eiUldDAgMBAAGjWTBXMA4GA1UdDwEB/wQEAwICpDAP
BgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBTwVdjA5FDji5vgc9MPG3Gs/lN5nzAV
BgNVHREEDjAMggprdWJlcm5ldGVzMA0GCSqGSIb3DQEBCwUAA4IBAQB2CljudYA0
C16SfF0221RhE2Zp3oZerA6pk+HOgEnjsCy/WzSaZSMXW1Iw6xUBE+xQ7gmrX+0u
y+MRHIv0rdqasFinWUw44sjgbJsKNfcZRD2TAIEFzmXsqwuGaK7lN8yxp/9qZIsW
XU+Y3THaiANulmKCO2AnXMkEeX2TCt9PDmKo6phXtgXrU2LIDy+N3lGwkfxKGFJW
0F3x4DkJ6Hrqw3aqPata4iFMJdiyptyLotvkxIo7GCoQ5OSh4yZ4Rhkr9stcfGZ+
z9Op7jVLZL2duO0IFiTsfds61HuwrQjo3Ro9tysPOqjhxh7rzmdrgHdNumaECLJC
QQjrosyd2Uj6
-----END CERTIFICATE-----

docker ps -a | grep kube-apiserver

crictl ps -a | grep kube-apiserver

cat /etc/kubernetes/manifests/etcd.yaml

apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.13.200.9:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.13.200.9:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.13.200.9:2380
    - --initial-cluster=controlplane=https://192.13.200.9:2380
    - --key-file=/etc/kubernetes/pki/etcd/server.key
    - --listen-client-urls=https://127.0.0.1:2379,https://192.13.200.9:2379
    - --listen-metrics-urls=http://127.0.0.1:2381
    - --listen-peer-urls=https://192.13.200.9:2380
    - --name=controlplane
    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
    - --peer-client-cert-auth=true
    - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    - --snapshot-count=10000
    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.10-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /health?exclude=NOSPACE&serializable=true
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: etcd
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /health?serializable=false
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      type: DirectoryOrCreate
    name: etcd-data
status: {}

#Generate key
openssl genrsa -out jane.key 2048

#Send request to administrator
openssl req -new -key kane.key -subj "/CN=jane" -out jane.csr

cat jane.csr | base64

# To identify the new request for approval
kubectl get csr

kubectl certificate approve jane

#View certificate
kubectl get csr jane -o yaml

# decode base64
echo "LS0...Qo" | base64 --decode

#Controller manager is responsible for CSR-APROVING and CSR-SIGNING

#user1.yaml

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: myuser
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZVzVuWld4aE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTByczhJTHRHdTYxakx2dHhWTTJSVlRWMDNHWlJTWWw0dWluVWo4RElaWjBOCnR2MUZtRVFSd3VoaUZsOFEzcWl0Qm0wMUFSMkNJVXBGd2ZzSjZ4MXF3ckJzVkhZbGlBNVhwRVpZM3ExcGswSDQKM3Z3aGJlK1o2MVNrVHF5SVBYUUwrTWM5T1Nsbm0xb0R2N0NtSkZNMUlMRVI3QTVGZnZKOEdFRjJ6dHBoaUlFMwpub1dtdHNZb3JuT2wzc2lHQ2ZGZzR4Zmd4eW8ybmlneFNVekl1bXNnVm9PM2ttT0x1RVF6cXpkakJ3TFJXbWlECklmMXBMWnoyalVnald4UkhCM1gyWnVVV1d1T09PZnpXM01LaE8ybHEvZi9DdS8wYk83c0x0MCt3U2ZMSU91TFcKcW90blZtRmxMMytqTy82WDNDKzBERHk5aUtwbXJjVDBnWGZLemE1dHJRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBR05WdmVIOGR4ZzNvK21VeVRkbmFjVmQ1N24zSkExdnZEU1JWREkyQTZ1eXN3ZFp1L1BVCkkwZXpZWFV0RVNnSk1IRmQycVVNMjNuNVJsSXJ3R0xuUXFISUh5VStWWHhsdnZsRnpNOVpEWllSTmU3QlJvYXgKQVlEdUI5STZXT3FYbkFvczFqRmxNUG5NbFpqdU5kSGxpT1BjTU1oNndLaTZzZFhpVStHYTJ2RUVLY01jSVUyRgpvU2djUWdMYTk0aEpacGk3ZnNMdm1OQUxoT045UHdNMGM1dVJVejV4T0dGMUtCbWRSeEgvbUNOS2JKYjFRQm1HCkkwYitEUEdaTktXTU0xMzhIQXdoV0tkNjVoVHdYOWl4V3ZHMkh4TG1WQzg0L1BHT0tWQW9FNkpsYWFHdTlQVmkKdjlOSjVaZlZrcXdCd0hKbzZXdk9xVlA3SVFjZmg3d0drWm89Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  expirationSeconds: 86400  # one day
  usages:
  - client auth

cat akshay.csr | base64 -w 0

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQXppdDZobzNWY3Q1TmtlZEpJSUJrM1FBa0ZyaTUwZFRmR2lMOURqdXdiKy9PCk5rNlpvbWlHcVRYYWR1UDlVUVVUeU9UQkdvUnBuT2ltcjYvVTVmdUpsZFh3eFE1NTEwaDdEY0QxN3I5aXpDOXQKTmhYVEpqZnJVakVpZDIyRzRFYXpzVWQyUC9UQTBJeUVYcFZZclpKb1NST3hGYyt2WlVkc3ZTVjBUbEcrMllUUAo2enI2UW1DNUxYa0RCd3VHM1NEcXl1U28rcittTVYzYWtTdEFUUWJhcVo0R2NYano4MUt0R2Z0OVZpaVFYZXoyCk5WMUhkNkYwTS9FREI5UXF2QXlDU05kelFMOU0rQTdqUnpsbW5pVVJ5MzNLdEVjRjZkL0NmbXRrcGNFbmUwZWYKVS9tWXkwN3pLblJ2WVIvS3NiSFI0bTZiOFBnY2NyNVg5eCtZaGU1bnJ3SURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBS0NFV3Ivb1haRm80SmdRR21UKzNuSTZTZFpKWHgybTh3azVicFFIZlB6SVRYbjdJV0xDCmFmaGxYTHkxdVJXRkRYcS96Rkh4MDFlOERNNjBlTUxpek01bWF3M21sRG9iSEk5c29Tbm5YMEJ2ZU5hblVnMWwKM2hIR3poMEk4M2JrcGFMRTZjb0p0MUh5MkZTK0E5L0dHOXJ2dXNUc1lqVU9ieFdlWThRM2k1SVNCaVRNM3JjSgp4MWdhRHpzTHZIRFVPRDFKWjFWSW54bTJqYWQzeEEwYjRRZ0dXYU9xWmo3UnRRa2ErZWtMeXo2V2pKTFpzb0dxCk1oWWptYU1aeDU4Vk5FallyUjVYL0ZwRzJmTlZvTVhkZVlBUk9aRWpDUWNwaUtwVWlFbFRZbUtML0MzU2hJL2cKVStwditvTXRCL3ExU3d0d0JwUjRjMGxLV3RaRENsSDFYTzA9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
 
kubectl get csr
 
kubectl get csr agent-smith -o yaml

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  creationTimestamp: "2024-04-08T10:11:06Z"
  name: agent-smith
  resourceVersion: "2783"
  uid: 5f4c4804-baef-4a27-95da-bab676f720ab
spec:
  groups:
  - system:masters
  - system:authenticated
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1dEQ0NBVUFDQVFBd0V6RVJNQThHQTFVRUF3d0libVYzTFhWelpYSXdnZ0VpTUEwR0NTcUdTSWIzRFFFQgpBUVVBQTRJQkR3QXdnZ0VLQW9JQkFRRE8wV0pXK0RYc0FKU0lyanBObzV2UklCcGxuemcrNnhjOStVVndrS2kwCkxmQzI3dCsxZUVuT041TXVxOTlOZXZtTUVPbnJEVU8vdGh5VnFQMncyWE5JRFJYall5RjQwRmJtRCs1eld5Q0sKeTNCaWhoQjkzTUo3T3FsM1VUdlo4VEVMcXlhRGtuUmwvanYvU3hnWGtvazBBQlVUcFdNeDRCcFNpS2IwVSt0RQpJRjVueEF0dE1Wa0RQUTdOYmVaUkc0M2IrUVdsVkdSL3o2RFdPZkpuYmZlek90YUF5ZEdMVFpGQy93VHB6NTJrCkVjQ1hBd3FDaGpCTGt6MkJIUFI0Sjg5RDZYYjhrMzlwdTZqcHluZ1Y2dVAwdEliT3pwcU52MFkwcWRFWnB3bXcKajJxRUwraFpFV2trRno4MGxOTnR5VDVMeE1xRU5EQ25JZ3dDNEdaaVJHYnJBZ01CQUFHZ0FEQU5CZ2txaGtpRwo5dzBCQVFzRkFBT0NBUUVBUzlpUzZDMXV4VHVmNUJCWVNVN1FGUUhVemFsTnhBZFlzYU9SUlFOd0had0hxR2k0CmhPSzRhMnp5TnlpNDRPT2lqeWFENnRVVzhEU3hrcjhCTEs4S2czc3JSRXRKcWw1ckxaeTlMUlZyc0pnaEQ0Z1kKUDlOTCthRFJTeFJPVlNxQmFCMm5XZVlwTTVjSjVURjUzbGVzTlNOTUxRMisrUk1uakRRSjdqdVBFaWM4L2RoawpXcjJFVU02VWF3enlrcmRISW13VHYybWxNWTBSK0ROdFYxWWllKzBIOS9ZRWx0K0ZTR2poNUw1WVV2STFEcWl5CjRsM0UveTNxTDcxV2ZBY3VIM09zVnBVVW5RSVNNZFFzMHFXQ3NiRTU2Q0M1RGhQR1pJcFVibktVcEF3a2ErOEUKdndRMDdqRytocGtueG11RkFlWHhnVXdvZEFMYUo3anUvVERJY3c9PQotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - digital signature
  - key encipherment
  - server auth
  username: agent-x
status: {}



kubectl certificate deny agent-smith

kubectl delete csr agent-smith

curl https://my-kube-playground:6443/api/v1/pods --key admin.key --cert admin.crt --cacert ca.crt

kubectl get pods --server my-kube-playground:6443 --client-key admin.key --client-certificate admin.crt --certificate-authority ca.crt

kubectl get pods --kubeconfig config

# $HOME/.kube/config

--server my-kube-playground:6443 
--client-key admin.key 
--client-certificate admin.crt 
--certificate-authority ca.crt

#KubeConfig

apiVersion: v1
kind: Config
current-context:	my-kube-admin@my-kube-playground
clusters: 
-	name: my-kube-playground
	cluster:	
		certificate-authority:	ca.crt
		server: 	https://my-kube-playground:6443
		

contexts:
-	name:	my-kube-admin@my-kube-playground
	context:
		cluster: 	my-kube-playground
		user:	my-kube-admin
		
users:
-	name:	my-kube-admin
	user:	
		client-certificate:	admin.crt
		client-key: 	amdin.key
		
		
kubectl config view

#Can mention custom config
kubectl config view --kubeconfig=my-custom-config

#to change the current context for config
kubectl config use-context prod-user@production

echo $HOME

cat .kube/config

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJRjZuR2dSZHRFRHN3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBME1EZ3hNelV5TXpsYUZ3MHpOREEwTURZeE16VTNNemxhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUN4aWw2MzNyTC82Y0ZxRFJSSkpCMEZwWlZlcU5CZXBsWVN5UGZBNUNDTmp5MWxWY0J4ZGsvOTJaZlYKcVJXYzVGNm5mNE9FeEE1UTZ6WXJSZFNmcXh0aW5aMlVpUlJ2YjBMcFJNQXZpd0ZCcWtsa2lOOVN5TWFwZlJSdgoxREFvU1FSbGVOc3ZzZzNkQVJta1prbGdrcDJvMWRhTmZQYXRUdng5cmx6VmJNWjFWbVovbmsxcTFtVUluTnFiCnYwQ1gvdjBiSXg2V0JpeHRRTnRCT0RobjZ6K3BNazVEaE9vREhkQ3NZMkJ0SmV5NnFhN2lObjNnU01KUmh2TEIKV0hlbGE4V1J0SFVwOW5iYUVYWWVzY1JNcnd6enpQV3gzQ1Y3Mk9lL0syVUt0UWNOaUN5eWdhSFNzSE9NUURyVApRemIxMlU5MGw3VHBQUUJOYStRaDdSejFwcFlkQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJUV2lnK3NYVTNHbFpXTHdEeWhKOUM1a3diTnBUQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQmRPL1J2MkVWbQpUMUpIWUpQTDBFemJqT2dYenk0QVhOU05xWmdOdThZT0pZZDlMRXhyQXQ0Y2NQcms0MVE0eDI0aG9NSTM2ZDNYCmN3STRwM2hjNGN4ZFBHRngycHRRSTduY3d2MDBscFd4WFVSVmtIRnpuSkJnZ3hGYXBkRHVYMGEzV2wrN1oySnUKZ2FGVzAxV1JXTEpFWXpNT0RtS0pkS3VUK2lJcDVVdEM2N2dhcEdUWlFqMFdUZjlSWXVmNDVPOTlEanc5N2VNaApXd2MzNVcycU5lQmd3QUdMbS9oU0JZb3U4Nm9DQkVQaGpmYlNVdGtrN0NMVjh3cW02RTVWODdNVVFaNkhRckY1ClhtMk1UcHd1NTQ2dFVSK0crZ2tmdFFMa2d5T1ZEcWk5UkRZVmhiU2IxcWZKenUzRWpYby8xR0x5ck54TjR1T3kKZVdSMCtxbUNYYko0Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://controlplane:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLVENDQWhHZ0F3SUJBZ0lJUzgvSkx4THoyeHN3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBME1EZ3hNelV5TXpsYUZ3MHlOVEEwTURneE16VTNOREZhTUR3eApIekFkQmdOVkJBb1RGbXQxWW1WaFpHMDZZMngxYzNSbGNpMWhaRzFwYm5NeEdUQVhCZ05WQkFNVEVHdDFZbVZ5CmJtVjBaWE10WVdSdGFXNHdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFEeVNHa1UKaVJUMGlSNXJuYlVWK1ZmSG9oVlg3b0JWUGw1ZVNnakxEOEpKOG81ci9Idkc3NmZpbENhUEFCSUNXc1A1bW84eQpWaFJwWFFjaFlkOXZUcFNSajFnOUhpLzVEcDliNnE0aEpCeS9xM1FBOWNBUWFiRi9Mbyt0cnI1cnVkWXlseThSCjFhRWhqdldlaThvVzcybmx5SEs2Y2xvaUM1VWJvTndCaTBHeWNxL2JBZGtCWjQyQ2FNdmUwcXdDUGNHSkhZYzAKZkl1N05aRjErZktOOXFqY1JrSW4vU3ZtdGdxUEhIQ0JsWWZMR2JBMUM0WFg0SWhFSWIyRzg4ajdwQjYwenFMeQpKRlQ4NEEvazhHTzEzM0xaWjI3MjBnVlZseUlBYnFCUjJqRndLRzhNdVE1VW5IT2dkSGdqdjBhL1dGNUxSeEdxCjk5bkI0WWlNbUFrS0l2MVhBZ01CQUFHalZqQlVNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUsKQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjhHQTFVZEl3UVlNQmFBRk5hS0Q2eGRUY2FWbFl2QQpQS0VuMExtVEJzMmxNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUJuMEpNMEoxeDc3WUhXVjlEOWU5VnpGTVNBCmdhSlNpeWtHR01CZE9PNW5iSUEwVHQvM0cvazFsQWxYVFk2dGxSVytZK1gzMHNCekVhcWNRd01LK1BOOHNlMHMKUm9weWNPVFUxWm4yRDVSNnkyV2h0VzlSK0NSU3hDVlBKVlhkbFFqeURzNUVwMkpXY3h2Q3h1VnZLb0p5V1dnRAp0bmxSd3JVV0VkSUtOU3ZUeGRaRjFJZ0lOTnFkdk9vai9HaWV2MHh3SVdBZzVuek5XVWp1OWJ2WFp4RThLVEplCm1JZGdYdnJkR3l1Q3cwb3BnOEM1UlhrTGs5ZUF4RkNBMTRQSC9Ya0JBdThvM1VwZzBxNnRoK3RWUXhwbjh6UjYKQ0pLSkRiSTQvTEs5YjZBdUF4RFB1RENOMEFGQ0E1dDJnY0hic3BZNEs3QnNMODV0ZTVLZ2Zrc0ljOEtpCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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

cat /root/my-kube-config 

apiVersion: v1
kind: Config

clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key

current-context: test-user@development
preferences: {}


kubectl config use-context research --kubeconfig /root/my-kube-config

#move file to default config file location
mv /root/my-kube-config /root/.kube/config

kubectl config view

vi /root/.kube/config

#To get the version 
curl https://kube-master:6443/version

#To get pods
curl https://kube-master:6443/api/v1/pods

# To Get List of api's
curl https://localhost:6443 -k

# To get the names of supported resource groups
curl https://localhost:6443 k | grep "name"

curl https://localhost:6443 -k --key admin.key --cert admin.crt cacert ca.crt

Authorization Nodes
1) Node
2) ABAC (Attribute based authorization)
3) RBAC (Role based authorization)
4) WEBHOOK (Role agent)
5) AlwaysAllow
6) AlwaysDeny

# --authorization-mode=Node,RBAC,Webhook


#developer-role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata: 
	name: developer
rules:
-	apiGroups: [""]	               # keep for core group
	resources: ["pods"]
	verbs: ["list", "get" , "create", "update", "delete"]
	namespaces: ["blue", "orange"]
-	apiGroups: [""]
	resources: ["ConfigMap"]
	verbs: ["create"]

kubectl create -f developer-role.yaml 

#devuser-developer-binding.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata: 
	name: devuser-developer-binding	
subjects: 
-	kind: User
	name: dev-user
	apiGroup: rbac.authorization.k8s.io
roleRef:
	kind: Role
	name: developer
	apiGroup: rbac.authorization.k8s.io
	
kubectl get roles

kubectl get rolbindings

kubectl describe role developer

kubectl describe rolebinding devuser-developer-binding

#Check Access
kubectl auth can-i create deployments

kubectl auth can-i delete nodes

kubectl auth can-i create deployments --as dev-user

kubectl auth can-i create nodes --as dev-user

kubectl auth can-i create pods --as dev-user --namespace test

cat /etc/kubernetes/manifests/kube-apiserver.yaml

kubectl get roles

# Count roles that exist in all namespaces
kubectl get roles -A --no-headers | wc -l

kubectl describe role kube-proxy -n kube-system

kubectl get rolebindings -n kube-system 

kubectl describe rolebindings kube-proxy -n kube-system

kubectl get pods --as dev-user

kubectl create role -h

kubectl create role developer --verb=list,create,delete --resource=pods

kubectl create rolebinding dev-user-binding --role=developer --user=dev-user

kubectl --as dev-user get pod dark-blue-app -n blue

kubectl describe role developer -n blue

kubectl edit role role developer -n blue

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2024-04-11T13:19:57Z"
  name: developer
  namespace: blue
  resourceVersion: "790"
  uid: b0d03dd9-7edd-41eb-a3c6-b1074964c9ba
rules:
- apiGroups:
  - ""
  resourceNames:
  - dark-blue-app
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete
  

kubectl --as dev-user create deployment nginx --image=nginx -n blue

kubectl edit role developer

# namespace blue
kubectl edit role developer -n blue

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: "2024-04-11T13:45:19Z"
  name: developer
  namespace: default
  resourceVersion: "2800"
  uid: c2de6a35-241e-431d-b8ed-8cea90986311
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - create
  - delete
- apiGroups:
  - "apps"
  resources:
  - deployment
  verbs:
  - list
  - create
  - delete
  
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Namespace Vs ClusterScope

Namespaced: Pods, replicasets, jobs, deployments, services, secrets, roles, rolebindings, configmaps and PVC.

Cluster Scoped: nodes, PV, clusterRoles, clusterrolebindings, certificatesigningrequests and namespaces.

kubectl api-resources --namespaced=true

kubectl api-resources --namespaced=false

#cluster-admin-role.yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata: 
	name: cluster-administrator
rules:
-	apiGroups:[""]
	resources: ["nodes"]
	verbs:["list","get","create", "delete"]
	
kubectl create -f cluster-admin-role.yaml

#cluster-admin-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
	name: cluster-admin-role-binding
subjects:
-	kind: User
	name: cluster-admin
	apiGroup: rbac.authorization.k8s.io/v1
roleRef:
	kind: ClusterRole
	name: cluster-administrator
	apiGroup: rbac.authorization.k8s.io
	
kubectl create -f cluster-admin-role-binding.yaml

kubectl get clusterRoles --no-headers | wc -l

kubectl get clusterrolebindings --no-headers | wc -l

kubectl get clusterrolebindings | grep cluster-admin 

kubectl describe clusterrolebindings cluster-admin

kubectl describe clusterrole cluster-admin

kubectl get nodes --as michelle 

kubectl create clusterrole michelle-role --verb=get,list,watch --resource=nodes

kubectl create clusterrolebinding michelle-role-binding --clusterrole=michelle-role --user=michelle

kubectl api-resources
NAME                              SHORTNAMES   APIVERSION                        NAMESPACED   KIND
bindings                                       v1                                true         Binding
componentstatuses                 cs           v1                                false        ComponentStatus
configmaps                        cm           v1                                true         ConfigMap
endpoints                         ep           v1                                true         Endpoints
events                            ev           v1                                true         Event
limitranges                       limits       v1                                true         LimitRange
namespaces                        ns           v1                                false        Namespace
nodes                             no           v1                                false        Node
persistentvolumeclaims            pvc          v1                                true         PersistentVolumeClaim
persistentvolumes                 pv           v1                                false        PersistentVolume
pods                              po           v1                                true         Pod
podtemplates                                   v1                                true         PodTemplate
replicationcontrollers            rc           v1                                true         ReplicationController
resourcequotas                    quota        v1                                true         ResourceQuota
secrets                                        v1                                true         Secret
serviceaccounts                   sa           v1                                true         ServiceAccount
services                          svc          v1                                true         Service
mutatingwebhookconfigurations                  admissionregistration.k8s.io/v1   false        MutatingWebhookConfiguration
validatingwebhookconfigurations                admissionregistration.k8s.io/v1   false        ValidatingWebhookConfiguration
customresourcedefinitions         crd,crds     apiextensions.k8s.io/v1           false        CustomResourceDefinition
apiservices                                    apiregistration.k8s.io/v1         false        APIService
controllerrevisions                            apps/v1                           true         ControllerRevision
daemonsets                        ds           apps/v1                           true         DaemonSet
deployments                       deploy       apps/v1                           true         Deployment
replicasets                       rs           apps/v1                           true         ReplicaSet
statefulsets                      sts          apps/v1                           true         StatefulSet
selfsubjectreviews                             authentication.k8s.io/v1          false        SelfSubjectReview
tokenreviews                                   authentication.k8s.io/v1          false        TokenReview
localsubjectaccessreviews                      authorization.k8s.io/v1           true         LocalSubjectAccessReview
selfsubjectaccessreviews                       authorization.k8s.io/v1           false        SelfSubjectAccessReview
selfsubjectrulesreviews                        authorization.k8s.io/v1           false        SelfSubjectRulesReview
subjectaccessreviews                           authorization.k8s.io/v1           false        SubjectAccessReview
horizontalpodautoscalers          hpa          autoscaling/v2                    true         HorizontalPodAutoscaler
cronjobs                          cj           batch/v1                          true         CronJob
jobs                                           batch/v1                          true         Job
certificatesigningrequests        csr          certificates.k8s.io/v1            false        CertificateSigningRequest
leases                                         coordination.k8s.io/v1            true         Lease
endpointslices                                 discovery.k8s.io/v1               true         EndpointSlice
events                            ev           events.k8s.io/v1                  true         Event
flowschemas                                    flowcontrol.apiserver.k8s.io/v1   false        FlowSchema
prioritylevelconfigurations                    flowcontrol.apiserver.k8s.io/v1   false        PriorityLevelConfiguration
helmchartconfigs                               helm.cattle.io/v1                 true         HelmChartConfig
helmcharts                                     helm.cattle.io/v1                 true         HelmChart
addons                                         k3s.cattle.io/v1                  true         Addon
etcdsnapshotfiles                              k3s.cattle.io/v1                  false        ETCDSnapshotFile
nodes                                          metrics.k8s.io/v1beta1            false        NodeMetrics
pods                                           metrics.k8s.io/v1beta1            true         PodMetrics
ingressclasses                                 networking.k8s.io/v1              false        IngressClass
ingresses                         ing          networking.k8s.io/v1              true         Ingress
networkpolicies                   netpol       networking.k8s.io/v1              true         NetworkPolicy
runtimeclasses                                 node.k8s.io/v1                    false        RuntimeClass
poddisruptionbudgets              pdb          policy/v1                         true         PodDisruptionBudget
clusterrolebindings                            rbac.authorization.k8s.io/v1      false        ClusterRoleBinding
clusterroles                                   rbac.authorization.k8s.io/v1      false        ClusterRole
rolebindings                                   rbac.authorization.k8s.io/v1      true         RoleBinding
roles                                          rbac.authorization.k8s.io/v1      true         Role
priorityclasses                   pc           scheduling.k8s.io/v1              false        PriorityClass
csidrivers                                     storage.k8s.io/v1                 false        CSIDriver
csinodes                                       storage.k8s.io/v1                 false        CSINode
csistoragecapacities                           storage.k8s.io/v1                 true         CSIStorageCapacity
storageclasses                    sc           storage.k8s.io/v1                 false        StorageClass
volumeattachments                              storage.k8s.io/v1                 false        VolumeAttachment
ingressroutes                                  traefik.containo.us/v1alpha1      true         IngressRoute
ingressroutetcps                               traefik.containo.us/v1alpha1      true         IngressRouteTCP
ingressrouteudps                               traefik.containo.us/v1alpha1      true         IngressRouteUDP
middlewares                                    traefik.containo.us/v1alpha1      true         Middleware
middlewaretcps                                 traefik.containo.us/v1alpha1      true         MiddlewareTCP
serverstransports                              traefik.containo.us/v1alpha1      true         ServersTransport
tlsoptions                                     traefik.containo.us/v1alpha1      true         TLSOption
tlsstores                                      traefik.containo.us/v1alpha1      true         TLSStore
traefikservices                                traefik.containo.us/v1alpha1      true         TraefikService
ingressroutes                                  traefik.io/v1alpha1               true         IngressRoute
ingressroutetcps                               traefik.io/v1alpha1               true         IngressRouteTCP
ingressrouteudps                               traefik.io/v1alpha1               true         IngressRouteUDP
middlewares                                    traefik.io/v1alpha1               true         Middleware
middlewaretcps                                 traefik.io/v1alpha1               true         MiddlewareTCP
serverstransports                              traefik.io/v1alpha1               true         ServersTransport
serverstransporttcps                           traefik.io/v1alpha1               true         ServersTransportTCP
tlsoptions                                     traefik.io/v1alpha1               true         TLSOption
tlsstores                                      traefik.io/v1alpha1               true         TLSStore
traefikservices                                traefik.io/v1alpha1               true         TraefikService

kubectl create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list,create,get,watch

kubectl create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list,create,get,watch

kubectl get clusterrole storage-admin -o yaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: "2024-04-13T10:15:39Z"
  name: storage-admin
  resourceVersion: "1582"
  uid: a9bcb5dd-4568-4d99-a3b6-11bf1152c27e
rules:
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - list
  - create
  - get
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - list
  - create
  - get
  - watch
  
kubectl create clusterrolebinding michelle-storage-admin --user=michelle --clusterrole=storage-admin

kubectl get storageclasses --as michelle


---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
kubectl create serviceaccount dashboard-sa

kubectl get serviceaccount

kubectl describe serviceaccount dashboard-sa

kubectl describe secret dashboard-sa token-kbbdm

curl https://192.168.56.70:6443/api -insecure --header "Authorization: Bearer ebvjwefu...."

kubectl exec -it my-kubernetes-dashboard -ls /var/run/secrets/kubernetes.io/serviceaccount

#to view token
kubectl exec -it my-kubernetes-dashboard -ls /var/run/secrets/kubenetes.io/serviceaccount/token

#pod-definition.yml
apiVersion: v1
kind: Pod
metadata: 
	name: my-kubernetes-dashboard
spec:
	containers:
		- 	name: my-kubernetes-dashboard
			image: my-kubernetes-dashboard
	serviceAccountName:	dashboard-sa
	
# autoMountServiceAccountTaken: false

#decode the token
jq -R 'split(".") | select(length > 0) | .[0],.[1] | @base64d | fromjson' <<< eykjbsxdu....

kubectl get pod my-kubernetes-dashboard -o yaml


#v1.24 onwards we need to specifically create token after service is created.
kubectl create token dashboard-sa

#v1.24 onwards
#secret-definition.ynl

apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
	name: mysecretname
	
	annotations: 
		kubernetes.io/service-account.name: dashboard-sa
		
kubectl get sa


# cat dashboard-sa-role-binding.yaml

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: dashboard-sa # Name is case sensitive
  namespace: default
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io

# cat pod-reader-role.yaml

  kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - watch
  - list
  
  
kubectl get deployment web-dashboard -o yaml > dashboard.yaml







# cat dashboard.yaml 

apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2024-04-17T10:25:54Z"
  generation: 1
  name: web-dashboard
  namespace: default
  resourceVersion: "829"
  uid: 42b06512-a8c2-47da-8e28-a952e4e66d45
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - env:
        - name: PYTHONUNBUFFERED
          value: "1"
        image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2024-04-17T10:25:59Z"
    lastUpdateTime: "2024-04-17T10:25:59Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2024-04-17T10:25:54Z"
    lastUpdateTime: "2024-04-17T10:25:59Z"
    message: ReplicaSet "web-dashboard-74cbcd9494" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
  

docker login private-register.io 

docker run private-register.io/apps/internal-app

kubectl create secret docker-registry regcred --docker-server= private-register.io --docker-username=registry-user --docker-password=registry-password --docker-email=registry-user@org.com


# nginx-pod.yaml

apiVersion: v1
kind: Pod
metadata: 
	name: nginx-pod
spec: 
	containers: 
	- 	name: nginx
		image: private-register.io/apps/internal-app
	imagePullSecrets:
	-	name:	regcred


kubectl get deploy


# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2024-04-18T13:08:37Z"
  generation: 1
  labels:
    app: web
  name: web
  namespace: default
  resourceVersion: "1571"
  uid: 9ce89941-516a-4e3c-8a11-e7cc3ec7f760
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: web
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: web
    spec:
      containers:
      - image: myprivateregistry.com:5000/nginx:alpine
        imagePullPolicy: IfNotPresent
		name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2024-04-18T13:08:42Z"
    lastUpdateTime: "2024-04-18T13:08:42Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2024-04-18T13:08:37Z"
    lastUpdateTime: "2024-04-18T13:08:42Z"
    message: ReplicaSet "web-7597ddbc86" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 2
  replicas: 2
 
#documentation for secret command 
kubectl create secret docker-registry -h


kubectl create secret docker-registry private-reg-cred --docker-server=myprivateregistry.com:5000 --docker-
username=dock_user --docker-password=dock_password --docker-email=dock_user@myprivateregistry.com


apiVersion: v1
kind: Pod
metadata:
  name: private-reg
spec:
  containers:
  - name: private-reg-container
    image: <your-private-image>
  imagePullSecrets:
  - name: regcred


docker run ubuntu sleep 3600

ps aux

docker run --user=1000 ubuntu sleep 3600

Linux Capabilities
/usr/include/linux/capability.h

docker run --cap-add MAC_ADMI ubuntu

docker run --cap-drop kill ubuntu

docker run --privileged ubuntu 

docker build -t my-ubuntu-image

#Containers are encapsulated in pods

#Security Context
#Security at pod level

apiVersion: v1
kind: Pod
metadata: 	
	name:	web-pod
spec:	
	securityContext:
		runAsUser: 1000
	containers:
		-	name: ubuntu
			image: ubuntu
			command: ["sleep", "3600"]
			
#Security at container level

apiVersion: v1
kind: Pod
metadata: 	
	name:	web-pod
spec:	
	containers:
		-	name: ubuntu
			image: ubuntu
			command: ["sleep", "3600"]
			securityContext:
				runAsUser: 1000
			capabilities:
				add: ["MAC_ADMIN"]
				
kubectl exec ubuntu-sleeper -- whoami

kubectl get pod ubuntu-sleeper -o yaml > ubuntu-sleeper.yaml

vi multi-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002

  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]
	 
kubectl delete pod ubuntu-sleeper --force

kubectl apply -f ubuntu-sleeper.yaml

# The incoming traffic from the users is an ingress traffic
# The outgoing request to the app server is egress traffic

# policy-defintion.yaml
apiVersion: netwroking.k8s.io/v1
kind: NetworkPolicy
metadata:
	name: db-policy
spec: 
	podSelector: 
		matchLabels: 
			role: db
	policyTypes:
	-	Ingress
	ingress:
	- 	from:
		-	podSelector:
				matchLabels: 
					name: api-pod
		
		ports:
		- 	protocol: TCP
			port: 3306
			
kubectl apply -f policy-defintion.yaml

Solutions that support Netowrk policies
-	Kube-router
-	Calico
-	Romana
-	Weave-net

Solutions that do not support Network Policies
- Flannel



apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata: 
	name: 	db-policy
spec:
	podSelector:
		matchLabels:
			role:	db
	policyTypes:
	-	Ingress
	
	ingress:
	-	from:
		-	podSelector:
				matchLabels:
					name:	api-pod
			namespaceSelector:
				matchLabels:
					name: prod
		-	ipBlock:
				cidr: 	192.168.5.10/32
		
		ports:
		-	protocol:	TCP
			port: 3306
			
			

apiVersion:	networking.k8s.io/v1
kind: NetworkPolicy
metadata:
	name:	db-policy
spec:
	podSelector:
		matchLabels:
			name:	db
	
	policyTypes:
	- Ingress
	- Egress
	
	ingress:
	-	from:
		-	podSelector:
				matchLabels:
					name: api-pod
		ports:
		-	protocol:	TCP
			port:	3306
			
	egress:
	-	to:
		-	ipBlock:
				cidr:	192.168.10.5/32
		ports:
		-	protocol: 	TCP
			port:	80

Kubectx:

With this tool, you don't have to make use of lengthy kubectl config commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.


Installation:

    sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
    sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx


Syntax:

To list all contexts:

    kubectx


To switch to a new context:

    kubectx <context_name>


To switch back to previous context:

    kubectx -


To see current context:

    kubectx -c



Kubens:

This tool allows users to switch between namespaces quickly with a simple command.

Installation:

    sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
    sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens


Syntax:

To switch to a new namespace:

    kubens <new_namespace>


To switch back to previous namespace:

    kubens -


kubectl get networkpolicies


kubectl get netpol

kubectl describe netpol payroll-policy

Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal
  Not affecting egress traffic
  Policy Types: Ingress


cat internal-policy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector: 
        matchLabels:
          name: payroll
    ports:
      - protocol: TCP
        port: 8080

  - to: 
    - podSelector:
        matchLabels:
          name: mysql
    ports:
      - protocol: TCP
        port: 3306
		
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#Storage 
# Docker Storage : Volume Driver and Container Storage
#File System at: /var/lib/docker

#Docker File
From Ubuntu

RUN apt-get update && apt-get -y install

RUN pip install flask flask-mysql

COPY . /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask



docker build Dockerfile -t my-custom-app

#Image Layers(Read Only layers 1-5)
# Layer 1: Base Ubuntu Layer ->120MB

# Layer 2: Changes in apt packages ->306MB(Stores only changes from previous layer) 

# Layer 3: Changes in pip packages ->6.3MB

# Layer 4: Source Code  ->229B

# Layer 5: Update Entrypoint ->0B

docker run my-custom-app

#Layer 6: Container Layer (Read and Writable layer)


docker volume create data_volume

# /var/lib/docker/volumes/data_volume

#Docker Volume mounting
docker run -v data_volume:/var/lib/mysql mysql

docker run -v data_volume2:/var/lib/mysql mysql        #(data_volume2 is created automatically if not created earlier)

#example of bind mount. From any location of the host
docker run -v /data/mysql:/var/lib/mysql mysql

docker run --mount type=bind, source=/data/mysql, target=/var/lib/mysql mysql

#Storage Drivers
AUFS (default for Ubuntu OS)
ZFS
BTRFS
DeviceMapper
Overlay
Overlay2

#Volume Drivers
Local
Azure File Storage
Convoy
Digital Ocean Block Storage
Flocker
gce-docker
GlusterFS
NetApp
RexRay
PortWorx
VMware vSphere Storage

docker run -it --name mysql --volume-driver rexray/ebs --mount src=ebs-vol,target=/var/lib/mysql mysql    #choose specify volume driver on EBS where data is persisted. The data is safe overtherw even after container has shutdown.

apiVersion: v1
kind: Pod
metadata: 
	name: random-number-generator
spec:
	containers:
	-	image: alpine
		name: alpine
		command: ["/bin/sh","-c"]
		args: ["shut -i 0-100 -n 1 >> /opt/numnber.out;"]
		volumeMounts:
		-	mounthPath: /opt
			name: data-volume
	volumes:
	-	name: data-volume
		hostPath:
			path: /data
			type: Directory




#eg to use AWS EDS volume to store data
volumes:
-	name: data-volume
	awsElasticBlockStore:
		volumeId: <volume-id>
		fsType: ext4
		
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#Persistent Volume Claim (PVC)

#pv-defintion.yaml
apiVersion: v1
kind: PersistentVolume
metadata: 
	name: 	pv-vol1
spec:
	accessModes:
		-	ReadWriteOnce
		
	capacity:
		storage: 1Gi
	hostpath:
		path: /tmp/data

kubectl -f create pv-definition.yaml

kubectl get persistentVolume

#pv-defintion.yaml
apiVersion: v1
kind: PersistentVolume
metadata: 
	name: 	pv-vol1
spec:
	accessModes:
		-	ReadWriteOnce
		
	capacity:
		storage: 1Gi
	awsElasticBlockStore:
		volume-id: 	<volume-id>
		fsType: 	ext4
		
#Binding ways

selector:
	matchLabels:
		name: 	my-pv
		

labels:
	name:	my-pv
	
	
#pvc-defintion.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:	
	name:	myclaim
spec:	
	accessmode:
		-	ReadWriteOnce
	resources:
		requests:
			storage:	500Mi

kubectl create -f pvc-defintion.yaml

kubectl get persistentvolumeclaim

kubectl delete persistentvolumeclaim my-claim

#	persistentVolumeReclaimPolicy: Retain(by default)/Delete/Recycle


apiVersion: v1
kind: Pod
metadata:
	name: mypod
spec:
    containers:
      - name: myfrontend
        image: nginx
        volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
    volumes:
      - name: mypd
        persistentVolumeClaim:
        claimName: myclaim

#To view a file within a pod, exec command
kubectl exec webapp -- cat /log/app.log

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi 
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log
	
kubectl get pv

cat pvc.yaml 

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi
	  

kubectl replace --force -f pvc.yaml

gcloud beta compute disks create --size 1GB --region us-east1 pd-disk

#pv-definition.yaml

apiVersion: v1
kind: PersistenVolume
metadata:
	name: pv-vol1
spec:
	accessmode:
		-	ReadWriteOnce
	capacity:
		storage:	500Mi
	gcePersistentDisk:	
		pdName:	pd-disk
		fsType:	ext4
		
#Storage Class
#sc-definition.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
	name: google-storage
provisioner:	kubernetes.io/gce-pd


#pvc-defintion.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:	
	name:	myclaim
spec:	
	accessmode:
		-	ReadWriteOnce
	storageClassName:	google-storage
	resources:
		requests:
			storage:	500Mi
			
#sc-definition.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
	name: google-storage
provisioner:	kubernetes.io/gce-pd

parameters:
	type:	pd-standard | pd-ssd
	replication-type:	none | regional-pd
	
	
kubectl get storageclass

kubectl get sc

#cat pvc.yaml 

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  storageClassName: local-storage
  

kubectl run nginx --image=nginx:alpine --dry-run=client -o yaml > nginx.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx:alpine
    name: nginx
    resources: {}
    volumeMounts:
      - mountPath: "/var/www/html"
        name: local-pvc-volume
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
    - name: local-pvc-volume
      persistentVolumeClaim:
        claimName: local-pvc
status: {}

vi delayed-volume-sc.yaml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Networking
 
ip link

ip addr add 192.168.1.10/24 dev eth0

ip route add 192.168.2.0/24 via 192.168.1.1

route

ip route add 192.168.1.0/24 via 192.168.2.1

ip route add default via 192.168.2.1

ip route add 0.0.0.0 via 192.168.2.1

ip route add 192.168.2.0/24 via 192.168.1.6

ip route add 192.168.1.0/24 via 192.168.2.6

#to check if the host can forward the request to another network, we need verify the flag in the file at below location.
# By default the value os set to zero.
cat /proc/sys/net/ipv4/ip_forward

# to set the value to 1 use command given below
echo 1 > /proc/sys/net/ipv4/ip_forward

#Changes are valid only till restart.
#The value are set temporalily. To persist the setting value. We need to persist these values in etc/sys/control.conf file
net.ipv4.ip_forward = 1


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#DNS

#Name resolution for local use. Local file for hosts is read first before DNS server by default. The setting can be changes in file /etc/nsswitch.conf
cat >> /etc/hosts
192.168.1.11 		db

ping db

hostname

ssh db


#DNS server to resolve host name
cat /etc/resolve.conf
nameserver 		192.168.1.100

cat /etc/nsswitch.conf
#console output
......
hosts:	files dns
......

cat >> /etc/resolve.conf
nameserver 	192.168.1.100
nameserver	8.8.8.8              # well known DNS server

# Can be use to query from DNS server
nslookup www.google.com

# It return more details than nslookup
Dig www.google.com
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#Create new network namespace
ip netns add red

ip netns add blue

ip netns

#Run the command in a namespace
ip netns exec red ip link 

ip netns exec red ip link

ip -n red link

arp

ip netns exec red arp

route 

ip netns exec red route 

#Create virtual ethernet pair or virtual cable. Also reffered as pipe.
ip link add veth-red type veth peer name veth-blue

#attch red interface to red namespace
ip link set veth-red netns red

ip link set veth-blue netns blue

#assign ip address to interface attached to namespace
ip -n red addr add 192.168.15.1 dev veth-red

ip -n blue addr add 192.168.15.2 dev veth-blue 

#set the link up
ip -n red link set veth-red up

ip -n blue link set veth-blue up

ip netns exec red ping 192.168.15.2

#View the arp table
ip netns exec red arp

#arp table in the host
arp

#Create virtual network / virtual switch like LINUX switch , OPEN vSwitch

#add linux bridge
ip link add v-net-0 type bridge

ip link set dev v-net-0 up

#delete one end and the other end gets deleted automatically
ip -n red link del veth-red

ip link add veth-red type veth peer name veth-red-br

ip link add veth-blue type veth peer name vbeeth-blue-br

#attach links to namespace and the bridge
ip link set veth-red netns red

ip link set veth-red-br master v-net-0 

ip link set veth-blue netns blue

ip link set veth-blue-br master v-net-0

ip -n red addr add 192.168.15.1 dev veth-red

ip -n blue addr add 192.168.15.2 dev veth-blue

ip -n red link set veth-red up

ip -n blue set veth-blue up

ip addr add 192.168.15.5/24 dev v-net-0

#add route in blue name space
ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5

ip netns exec blue ping 192.168.1.3 

iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE

ip netns exec blue ping 8.8.8.8 

ip netns exec blue route

ip netns exec blue ip route add default via 192.168.15.5

#Port Forwarding
iptable -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2 -j DNAT


# While testing the Network Namespaces, if you come across issues where you can't ping one namespace from the other, make sure you set the NETMASK while setting IP Address. ie: 
# 192.168.1.10/24
ip -n red addr add 192.168.1.10/24 dev veth-red
  
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
#Docker

docker run --network none nginx

docker run --network host nginx

docker network ls

ip link add docker0 type bridge

docker run nginx

#list the namespace
ip netns

docker inspect 942d70e585b2

ip -n b3165c10a92b addr

docker run -p 8080:80 nginx

iptables -t nat -A PREROUTING -j DNAT --dport 8080 --to-destination 172.17.0.3:80

#list rule in ip tables
iptables -nvL -t nat

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
CNI-CONTAINER NETWORK INTERFACE
# It is a set of standards that define how programs should be developed to solve networking challenges in  a container runtime environment. The programs are reefered to as plugins.
# Bridge is plugin for CNI. CNI defines how the plugins should be developed.how container runtimes should invoke them ? 
bridge add 2e34dcf34 /var/run/netns/2e34dcf34

#CNI plugin examples:
BRIDGE, VLAN, IPVLAN, MACVLAN, WINDOWS, DHCP, host-local
Weave, Flannel, Cilium , NSX Vmware, 


Docker: CONTAINER NETWORK MODEL (CNM) does not implement CNI. CNM is slightly different from CNI.

kubectl get nodes -o wide

ip address

ip address show eth0

ip address show type bridge

ip route

#kube-scheduler listens on this port number 10259. 
netstat -npl | grep -i scheduler

# Etcd port can be found using following command. 2379
netstat -npl | grep -i etcd

# to check which port is connect most in the network.
netstat -npa | grep -i etcd 

#to count nuumber of times the port 2379 is connected.
netstat -npa | grep -i etcd | grep -i 2379 | wc -l   

netstat -npa | grep -i etcd

#kubelet.service
--network-plugin=cni
--cni-bin-dir=/opt/cni/bin
--cno-conf-dir=/etc/cni/net.d

ps -aux | grep kubelet

#path to check all the CNA supported plugins at location given below
ls /opt/cni/bin

#The CNI conflict diretory has a set of configuartion files. (10-bridge.conf)
ls /etc/cni/net.d 

cat /etc/cni/net.d/10-bridge.conf
{
	"cniVersion": "0.2.0",
	"name": "mynet",
	"type": "bridge",
	"isGateway": true,
	"ipMasq": true,
	"ipam": {
		"type":		"host-local",
		"subnet": 	"10.22.0.0/16",
		"routes":	[
			{	"dst":	"0.0.0.0/0"	}		
		]
	}
}


kubectl exec busybox ip route 

kubectl apply -f "https://cloud.weave.works/k8s/net?=k8s-version=$(kubectl version | base64 | tr -d '\n')"

#Inspect the kubelet service and identify the container runtime endpoint value is set for Kubernetes.
ps -aux | grep -i kubelet | grep container-runtime

cat /etc/cni/net.d/10-flannel.conflist 
{
  "name": "cbr0",
  "cniVersion": "0.3.1",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}


cat weave-daemonset-k8s.yaml 
apiVersion: v1
kind: List
items:
  - apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: weave-net
      labels:
        name: weave-net
      namespace: kube-system
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRole
    metadata:
      name: weave-net
      labels:
        name: weave-net
    rules:
      - apiGroups:
          - ''
        resources:
          - pods
          - namespaces
          - nodes
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - extensions
        resources:
          - networkpolicies
        verbs:
          - get
          - list
          - watch
      - apiGroups:
          - 'networking.k8s.io'
        resources:
          - networkpolicies
        verbs:
          - get
          - list
          - watch
      - apiGroups:
        - ''
        resources:
        - nodes/status
        verbs:
        - patch
        - update
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: weave-net
      labels:
        name: weave-net
    roleRef:
      kind: ClusterRole
      name: weave-net
      apiGroup: rbac.authorization.k8s.io
    subjects:
      - kind: ServiceAccount
        name: weave-net
        namespace: kube-system
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: Role
    metadata:
      name: weave-net
      namespace: kube-system
      labels:
        name: weave-net
    rules:
      - apiGroups:
          - ''
        resources:
          - configmaps
        resourceNames:
          - weave-net
        verbs:
          - get
          - update
      - apiGroups:
          - ''
        resources:
          - configmaps
        verbs:
          - create
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: RoleBinding
    metadata:
      name: weave-net
      namespace: kube-system
      labels:
        name: weave-net
    roleRef:
      kind: Role
      name: weave-net
      apiGroup: rbac.authorization.k8s.io
    subjects:
      - kind: ServiceAccount
        name: weave-net
        namespace: kube-system
  - apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: weave-net
      labels:
        name: weave-net
      namespace: kube-system
    spec:
      # Wait 5 seconds to let pod connect before rolling next pod
      selector:
        matchLabels:
          name: weave-net
      minReadySeconds: 5
      template:
        metadata:
          labels:
            name: weave-net
        spec:
          initContainers:
            - name: weave-init
              image: 'weaveworks/weave-kube:2.8.1'
              command:
                - /home/weave/init.sh
              env:
              securityContext:
                privileged: true
              volumeMounts:
                - name: cni-bin
                  mountPath: /host/opt
                - name: cni-bin2
                  mountPath: /host/home
                - name: cni-conf
                  mountPath: /host/etc
                - name: lib-modules
                  mountPath: /lib/modules
                - name: xtables-lock
                  mountPath: /run/xtables.lock
                  readOnly: false
          containers:
            - name: weave
              command:
                - /home/weave/launch.sh
              env:
                - name: IPALLOC_RANGE
                  value: 10.32.1.0/24
                - name: INIT_CONTAINER
                  value: "true"
                - name: HOSTNAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: spec.nodeName
              image: 'weaveworks/weave-kube:2.8.1'
              readinessProbe:
                httpGet:
                  host: 127.0.0.1
                  path: /status
                  port: 6784
              resources:
                requests:
                  cpu: 50m
              securityContext:
                privileged: true
              volumeMounts:
                - name: weavedb
                  mountPath: /weavedb
                - name: dbus
                  mountPath: /host/var/lib/dbus
                  readOnly: true
                - mountPath: /host/etc/machine-id
                  name: cni-machine-id
                  readOnly: true
                - name: xtables-lock
                  mountPath: /run/xtables.lock
                  readOnly: false
            - name: weave-npc
              env:
                - name: HOSTNAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: spec.nodeName
              image: 'weaveworks/weave-npc:2.8.1'
#npc-args
              resources:
                requests:
                  cpu: 50m
              securityContext:
                privileged: true
              volumeMounts:
                - name: xtables-lock
                  mountPath: /run/xtables.lock
                  readOnly: false
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          hostPID: false
          restartPolicy: Always
          securityContext:
            seLinuxOptions: {}
          serviceAccountName: weave-net
          tolerations:
            - effect: NoSchedule
              operator: Exists
            - effect: NoExecute
              operator: Exists
          volumes:
            - name: weavedb
              hostPath:
                path: /var/lib/weave
            - name: cni-bin
              hostPath:
                path: /opt
            - name: cni-bin2
              hostPath:
                path: /home
            - name: cni-conf
              hostPath:
                path: /etc
            - name: cni-machine-id
              hostPath:
                path: /etc/machine-id
            - name: dbus
              hostPath:
                path: /var/lib/dbus
            - name: lib-modules
              hostPath:
                path: /lib/modules
            - name: xtables-lock
              hostPath:
                path: /run/xtables.lock
                type: FileOrCreate
          priorityClassName: system-node-critical
      updateStrategy:
        type: RollingUpdate
		


wget https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

kubectl describe pod kube-proxy-p4jlg -n kube-system 
Name:                 kube-proxy-p4jlg
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Service Account:      kube-proxy
Node:                 controlplane/192.23.199.9
Start Time:           Mon, 29 Apr 2024 12:38:56 +0000
Labels:               controller-revision-hash=7958dbcbd9
                      k8s-app=kube-proxy
                      pod-template-generation=1
Annotations:          <none>
Status:               Running
IP:                   192.23.199.9
IPs:
  IP:           192.23.199.9
Controlled By:  DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  containerd://0bada94f2fe453523e90268f9e3aa4f9551403c2058c9f65c49a745dfcee2f97
    Image:         registry.k8s.io/kube-proxy:v1.29.0
    Image ID:      registry.k8s.io/kube-proxy@sha256:8da4de35c4929411300eb8052fdfd34095b6090ed0c8dbc776d58bf1c61a2c89
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Mon, 29 Apr 2024 12:38:58 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mzs42 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-api-access-mzs42:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  53m   default-scheduler  Successfully assigned kube-system/kube-proxy-p4jlg to controlplane
  Normal  Pulled     53m   kubelet            Container image "registry.k8s.io/kube-proxy:v1.29.0" already present on machine
  Normal  Created    53m   kubelet            Created container kube-proxy
  Normal  Started    53m   kubelet            Started container kube-proxy
  
 
#get all the configMaps 
kubectl get cm -n kube-system


kubectl describe cm kube-proxy -n kube-system

Name:         kube-proxy
Namespace:    kube-system
Labels:       app=kube-proxy
Annotations:  kubeadm.kubernetes.io/component-config.hash: sha256:a58bc934e3784937e225f9d601a6a87d49d87707757c3825c91340be984ace5a

Data
====
config.conf:
----
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
bindAddressHardFail: false
clientConnection:
  acceptContentTypes: ""
  burst: 0
  contentType: ""
  kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
  qps: 0
clusterCIDR: 10.244.0.0/16
configSyncPeriod: 0s
conntrack:
  maxPerCore: null
  min: null
  tcpBeLiberal: false
  tcpCloseWaitTimeout: null
  tcpEstablishedTimeout: null
  udpStreamTimeout: 0s
  udpTimeout: 0s
detectLocal:
  bridgeInterface: ""
  interfaceNamePrefix: ""
detectLocalMode: ""
enableProfiling: false
healthzBindAddress: ""
hostnameOverride: ""
iptables:
  localhostNodePorts: null
  masqueradeAll: false
  masqueradeBit: null
  minSyncPeriod: 0s
  syncPeriod: 0s
ipvs:
  excludeCIDRs: null
  minSyncPeriod: 0s
  scheduler: ""
  strictARP: false
  syncPeriod: 0s
  tcpFinTimeout: 0s
  tcpTimeout: 0s
  udpTimeout: 0s
kind: KubeProxyConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
metricsBindAddress: ""
mode: ""
nftables:
  masqueradeAll: false
  masqueradeBit: null
  minSyncPeriod: 0s
  syncPeriod: 0s
nodePortAddresses: null
oomScoreAdj: null
portRange: ""
showHiddenMetricsForVersion: ""
winkernel:
  enableDSR: false
  forwardHealthCheckVip: false
  networkName: ""
  rootHnsEndpointName: ""
  sourceVip: ""
kubeconfig.conf:
----
apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    server: https://controlplane:6443
  name: default
contexts:
- context:
    cluster: default
    namespace: default
    user: default
  name: default
current-context: default
users:
- name: default
  user:
    tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token

BinaryData
====

Events:  <none>



kubectl get pods -n kube-system

#Plugins DHCP and host-local manage the ip addresss for us

cat /etc/cni/net.d/net-script.conf

{
	"name": "mynet",
  "cniVersion": "0.2.0",
  "type": "net-script",
  "bridge": "cni0",
  "isGateway": true,
  "ipMasq": true,
  "ipam": [
    {
      "type": "host-local",
	  "subnet": "10.244.0.0/16",
      "routes": [
       { "dst": "0.0.0.0/0" }
      ]
    }
  ]
}


cat 10-weave.conflist 
{
    "cniVersion": "0.3.0",
    "name": "weave",
    "plugins": [
        {
            "name": "weave",
            "type": "weave-net",
            "hairpinMode": true
        },
        {
            "type": "portmap",
            "capabilities": {"portMappings": true},
            "snat": true
        }
    ]
}

kubectl get pods -n kube-system

kubectl get pods -n kube-system -o wide

ip add

kubectl run busybox --image=busybox --dry-run=client -o yaml -- sleep 1000 > busybox.yaml

cat busybox.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  nodeName: node01
  containers:
  - args:
    - sleep
    - "1000"
    image: busybox
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl exec busybox -- ip route

# Kube proxy uses userspace or iptable or ipvs to set rules. default is iptables if not set.

kubectl get pods -o wide

kubectl get service 

kube-api-server --service-cluster-ip-range ipNet 10.244.0.0/12     ( Default value is 10.0.0.0/24 )

iptables -L -t nat | grep db-service


# check logs of kube-proxy
cat /var/log/kube-proxy.log

ip add

kubectl get all --all-namespaces

kubectl logs weave-net-6h8hb -n kube-system

cat kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 192.22.121.9:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.22.121.9
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 192.22.121.9
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 192.22.121.9
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 192.22.121.9
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}

# --service-cluster-ip-range=10.96.0.0/12


kubectl logs kube-proxy-s9m5w -n kube-system

cat >> /etc/hosts

cat >> /etc/resolv.conf

cat /etc/coredns/Corefile

kubectl get configmap -n kube-system

kubectl get pod coredns-69f9c977-5hssq -n kube-system -o yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-05-01T13:36:01Z"
  generateName: coredns-69f9c977-
  labels:
    k8s-app: kube-dns
    pod-template-hash: 69f9c977
  name: coredns-69f9c977-5hssq
  namespace: kube-system
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: coredns-69f9c977
    uid: 7a9bd170-3281-4b0c-9b62-d30ed810261c
  resourceVersion: "450"
  uid: 5e3e285c-59e1-4b58-8761-4ec246378ad6
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - kube-dns
          topologyKey: kubernetes.io/hostname
        weight: 100
  containers:
  - args:
    - -conf
    - /etc/coredns/Corefile
    image: registry.k8s.io/coredns/coredns:v1.10.1
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 5
      httpGet:
        path: /health
        port: 8080
        scheme: HTTP
      initialDelaySeconds: 60
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 5
    name: coredns
    ports:
    - containerPort: 53
      name: dns
      protocol: UDP
    - containerPort: 53
      name: dns-tcp
      protocol: TCP
    - containerPort: 9153
      name: metrics
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /ready
        port: 8181
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      limits:
        memory: 170Mi
      requests:
        cpu: 100m
        memory: 70Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - NET_BIND_SERVICE
        drop:
        - ALL
      readOnlyRootFilesystem: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /etc/coredns
      name: config-volume
      readOnly: true
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-ksjzv
      readOnly: true
  dnsPolicy: Default
  enableServiceLinks: true
  nodeName: controlplane
  nodeSelector:
    kubernetes.io/os: linux
  preemptionPolicy: PreemptLowerPriority
  priority: 2000000000
  priorityClassName: system-cluster-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: coredns
  serviceAccountName: coredns
  terminationGracePeriodSeconds: 30
  tolerations:
  - key: CriticalAddonsOnly
    operator: Exists
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - configMap:
      defaultMode: 420
      items:
      - key: Corefile
        path: Corefile
      name: coredns
    name: config-volume
  - name: kube-api-access-ksjzv
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-05-01T13:36:18Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2024-05-01T13:36:03Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-05-01T13:36:24Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-05-01T13:36:24Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-05-01T13:36:03Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://db53372ac79d91e7e674980560cf35fcb72ecf3199d6c163754aab66dce5952e
    image: registry.k8s.io/coredns/coredns:v1.10.1
    imageID: registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e
    lastState: {}
    name: coredns
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-05-01T13:36:18Z"
  hostIP: 192.25.46.3
  hostIPs:
  - ip: 192.25.46.3
  phase: Running
  podIP: 10.244.0.3
  podIPs:
  - ip: 10.244.0.3
  qosClass: Burstable
  startTime: "2024-05-01T13:36:03Z"



kubectl describe configmap coredns -n kube-system

Name:         coredns
Namespace:    kube-system
Labels:       <none>
Annotations:  <none>

Data
====
Corefile:
----
.:53 {
    errors
    health {
       lameduck 5s
    }
    ready
    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }
    prometheus :9153
    forward . /etc/resolv.conf {
       max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}


BinaryData
====

Events:  <none>


kubectl describe svc web-service -n payroll 
Name:              web-service
Namespace:         payroll
Labels:            <none>
Annotations:       <none>
Selector:          name=web
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.100.85.186
IPs:               10.100.85.186
Port:              <unset>  80/TCP
TargetPort:        80/TCP
Endpoints:         10.244.0.4:80
Session Affinity:  None
Events:            <none>

#all namespaces
 kubectl get pods -A
 
 
 kubectl edit deploy webapp

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
  creationTimestamp: "2024-05-01T14:06:36Z"
  generation: 1
  labels:
    name: webapp
  name: webapp
  namespace: default
  resourceVersion: "2940"
  uid: 79a3b17c-0412-48cb-842d-a718df11f3c7
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp
    spec:
      containers:
      - env:
        - name: DB_Host
		value: mysql
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd
        image: mmumshad/simple-webapp-mysql
        imagePullPolicy: Always
        name: simple-webapp-mysql
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2024-05-01T14:06:50Z"
    lastUpdateTime: "2024-05-01T14:06:50Z"
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: "2024-05-01T14:06:36Z"
    lastUpdateTime: "2024-05-01T14:06:50Z"
    message: ReplicaSet "webapp-b9548974b" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  observedGeneration: 1
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1



kubectl exec hr -- nslookup mysql.payroll

Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   mysql.payroll.svc.cluster.local
Address: 10.102.70.224

# Load balancing solution NGINX, HAPROXY, traefik, Contour and Istio can be used for loadbalance. This is called ingress controller.
#Ingress is a layer seven load balancer built in to the kubernetes cluster that can be configureds using native kubernetes primitives.
#Ingress is implemented by kubernetes. It first deploys one the load balancing solutions and then specify a set of rules to configure ingress. It contains of two parts. First is 
#Ingress controller and second it ingress resources. Ingress resources are created using definition files.
#Ingress helps your users access your application using a single externally accessible URL. At the same implement SSL security as well.
#Ingress resources are set of rules. 

#Ingress Controller

#Deployment
apiVersion: extensions/v1betal
kind: Deployment
metadata: 
	name: 	nginx-ingress-controller
spec:
	replicas:	1
	selector:
		matchLabels:
			name: nginx-ingress
	template:
		metadata:
			labels:
				name: nginx-ingress
		spec:
			containers:
				-	name:	nginx-ingress-controller
					image:	quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
				
				args:
					-	/nginx-ingress-controller
				
				env: 
					-	name:	POD_NAME
						valueFrom:	
							fieldRef:
								fieldPath:	metadata.name
					-	name:	POD_NAMESPACE
						valueFrom:
							fieldRef:
								fieldPath:	matadata.namespace
				
				ports:
					-	name: 	http
						containerPort: 80
					-	name:	https:	
						containerPort:	443
						
					
#ConfigMap
apiVersion:	v1
kind: ConfigMap
metadata:
	name:	nginx-configuration
	
	
#Service
apiVersion:	v1
kind:	Service
metadata:
	name:	nginx-ingress
spec:
	type:	NodePort
	ports:	
	-	port:	80
		targetPort:	80
		protocol:	TCP
		name:	http
	-	port: 	443
		targetPort: 443
		protocol:	TCP
		name:	https
	selector:
		name:	nginx-Ingress
		
#Auth
apiVersion: v1
kind:	ServiceAccount
metadata:
	name:	nginx-ingress-serviceaccount
	
		
#ingress-wear.yaml
apiVersion:	extensions/v1beta1
kind:	Ingress
metadata:	
	name:	ingress-wear
spec:
	rules:
	-	paths:
		-	path:	/wear
			backend:
				serviceName:	wear-service
				servicePort:	80
		-	path:	/watch
			backend:
				serviceName:	watch-service
				servicePort:	80
				
#ingress-wear.yaml
apiVersion:	extensions/v1beta1
kind:	Ingress
metadata:	
	name:	ingress-wear
spec:
	rules:
	-	host:	wear.my-online-store.com
		http:
			paths:
			-	backend:
					serviceName:	wear-service
					servicePort:	80
		-	host:	watch.my-online-store.com
		http:
			paths:
			-	backend:
					serviceName:	watch-service
					servicePort:	80
				
kubectl describe ingress ingress-wear-watch
		
kubectl create -f ingress-wear.yaml

kubectl get ingress


kubectl describe ingress ingress-wear-watch

kubectl describe ingress ingress-wear-watch -n app-space 


kubectl edit ingress ingress-wear-watch -n app-space

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  creationTimestamp: "2024-05-05T10:16:20Z"
  generation: 1
  name: ingress-wear-watch
  namespace: app-space
  resourceVersion: "852"
  uid: c8f441be-2a08-45ab-874e-bc3663953395
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /watch
        pathType: Prefix
status:
  loadBalancer:
    ingress:
    - ip: 10.106.168.7
	
	
	
kubectl get deploy -n app-space

kubectl get svc -n app-space

kubectl edit ingress ingress-wear-watch -n app-space

#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
  creationTimestamp: "2024-05-05T10:16:20Z"
  generation: 2
  name: ingress-wear-watch
  namespace: app-space
  resourceVersion: "2758"
  uid: c8f441be-2a08-45ab-874e-bc3663953395
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /stream
        pathType: Prefix
      - backend:
          service:
            name: food-service
            port:
              number: 8080
        path: /eat
        pathType: Prefix
status:
  loadBalancer:
    ingress:
    - ip: 10.106.168.7
	
	
 kubectl create ingress ingress-pay -n critical-space  --rule="/pay=pay-service:8282"
 
 kubectl describe ingress -n critical-space
 
 kubectl edit ingress ingress-pay -n critical-space
 
 #
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  creationTimestamp: "2024-05-05T10:59:13Z"
  generation: 1
  name: ingress-pay
  namespace: critical-space
  resourceVersion: "4621"
  uid: bf550c33-afc8-4aec-814c-79a1376d1bda
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: pay-service
            port:
              number: 8282
        path: /pay
        pathType: Exact
status:
  loadBalancer:
    ingress:
    - ip: 10.106.168.7
	

kubectl create namespace ingress-nginx

kubectl create configmap ingress-nginx-controller -n ingress-nginx

kubectl create serviceaccount ingress-nginx -n ingress-nginx

kubectl create serviceaccount ingress-nginx-admission -n ingress-nginx

kubectl get roles -n ingress-nginx

kubectl get rolebindings -n ingress-nginx

kubectl describe role ingress-nginx -n ingress-nginx 

cat /root/ingress-controller.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
            containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodeport: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
  
#namespace : ingress-nginx



 cat /root/ingress-controller.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodeport: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - port: 80
      # By default and for convenience, the `targetPort` is set to
      # the same value as the `port` field.
      targetPort: 80
      # Optional field
      # By default and for convenience, the Kubernetes control plane
      # will allocate a port from a range (default: 30000-32767)
      nodePort: 30080
	  
	  
kubectl create ingress ingress-wear-watch -n app-space --rule="/wear=wear-service:8080" --rule="/watch=watch-serv
ice:8080"


#On Prem Solutions
# OpenShift , Cloud Foundry Container Runtime, Vmware CLoud PKS, Vagrant

#Hosted Kubernetes Solutions
# Google Container Engine(GKE), OpenShift Online, Azure Kubernetes Service, Amazon Elastic Container Service for Kubernetes.

kube-controller-manager --leader-elect true --leader-elect-lease-duration 15s --leader-elect-renew-deadline 10s --leader-elect-retry-period 2s 

# https://github.com/kodekloudhub/certified-kubernetes-administrator-course/

ls -la

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Deployment using kubeadm for 1.26 version of kubernetes.
#For each node we need to run these commands

#Container Runtimes
# https://v1-26.docs.kubernetes.io/docs/setup/production-environment/container-runtimes/
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

# Verify that the br_netfilter, overlay modules are loaded by running below instructions:
lsmod | grep br_netfilter
lsmod | grep overlay


# Verify that the net.bridge.bridge-nf-call-iptables, net.bridge.bridge-nf-call-ip6tables, net.ipv4.ip_forward system variables are set to 1 in your sysctl config by running below 
# instruction:
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward


# install Docker Engine
# Set up Docker's apt repository.
# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update


sudo apt-get install containerd.io

#check the status
systemctl status containerd

#get details of the init system
ps -p 1


#For all the nodes
sudo vi /etc/containerd/config.toml
#delete all the configuration the file.
#copy past the below configuration

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true

# run systemctl
sudo systemctl restart containerd


#Installing kubeadm, kubelet and kubectl
#https://v1-27.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

#Update the apt package index and install packages needed to use the Kubernetes apt repository:
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl

#Download the public signing key for the Kubernetes package repositories. The same signing key is used for all repositories so you can disregard the version in the URL:
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

#Add the appropriate Kubernetes apt repository. Please note that this repository have packages only for Kubernetes 1.27; for other Kubernetes minor versions, you need to change the #Kubernetes minor version in the URL to match your desired minor version (you should also check that you are reading the documentation for the version of Kubernetes that you plan to #install).
# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.27/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

#Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version:
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

# Only on master node
#Creating a cluster with kubeadm
# https://v1-27.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

sudo kubeadm reset
sudo systemctl restart kubelet

ip add
#sample output
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 02:8f:be:f2:49:93 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 metric 100 brd 10.0.2.255 scope global dynamic enp0s3
       valid_lft 78645sec preferred_lft 78645sec
    inet6 fe80::8f:beff:fef2:4993/64 scope link
       valid_lft forever preferred_lft forever
3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:3b:76:19 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.5/24 metric 100 brd 192.168.1.255 scope global dynamic enp0s8
       valid_lft 81751sec preferred_lft 81751sec
    inet6 2401:4900:1c9a:b443:a00:27ff:fe3b:7619/64 scope global dynamic mngtmpaddr noprefixroute
       valid_lft 85604sec preferred_lft 85604sec
    inet6 fe80::a00:27ff:fe3b:7619/64 scope link
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default
    link/ether 02:42:30:2b:e7:0f brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:30ff:fe2b:e70f/64 scope link
       valid_lft forever preferred_lft forever

#sudo kubeadm init --pod-network-cidr=10.244.0.0/16
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.1.5


#To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
  
#  https://kubernetes.io/docs/concepts/cluster-administration/addons/
# Install addon weanet
# https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
sudo kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
#sudo kubectl apply -f https://reweave.azurewebsites.net/k8s/v1.29/net.yaml

kubectl edit ds weave-net -n kube-system
  
# go worker nodes console and run these commands
  kubeadm join 10.0.2.15:6443 --token 34m7xz.n2qzhz5lbtmnz7t5 \
        --discovery-token-ca-cert-hash sha256:4293d337df64405f4906df5cab986287b5a2f9298f50cf67063c6c8e6dcb072b


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

kubeadm init --apiserver-advertise-address=192.17.109.12 --pod-network-cidr=10.244.0.0/16 --apiserver-cert-extra-sans=controlplane

#context config modified
kubectl config set-context --current --namespace=alpha

kubectl get deploy

kubectl get svc

curl http://localhost:30081

kubectl describe deploy webapp-mysql 
Name:                   webapp-mysql
Namespace:              alpha
CreationTimestamp:      Sat, 25 May 2024 10:34:59 +0000
Labels:                 name=webapp-mysql
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp-mysql
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp-mysql
  Containers:
   webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql-service
      DB_User:      root
      DB_Password:  paswrd
    Mounts:         <none>
  Volumes:          <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   webapp-mysql-b68bb6bc8 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  17m   deployment-controller  Scaled up replica set webapp-mysql-b68bb6bc8 to 1



kubectl edit svc mysql

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2024-05-25T10:34:59Z"
  name: mysql-service
  namespace: alpha
  resourceVersion: "792"
  uid: beaa9425-6308-4cb0-a290-912f73e07aae
spec:
  clusterIP: 10.43.255.220
  clusterIPs:
  - 10.43.255.220
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    name: mysql
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
  


kubectl delete svc mysql

kubectl create -f /tmp/kubectl-edit-922217273.yaml

kubectl edit svc mysql-service

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2024-05-25T10:57:47Z"
  name: mysql-service
  namespace: beta
  resourceVersion: "1299"
  uid: 5a996f45-cd08-4a2e-b050-2336c16edd59
spec:
  clusterIP: 10.43.22.173
  clusterIPs:
  - 10.43.22.173
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    name: mysql
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}




service kube-apiserver status

service kubelet status

kubectl logs kube-apiserver-master -n kube-system

sudo journalctl -u kube-apiserver

source <(kubectl completion bash)

alias k=kubectl
complete -o default -F __start_kubectl k

k describe deployments.apps 
Name:                   app
Namespace:              default
CreationTimestamp:      Sat, 25 May 2024 14:06:28 +0000
Labels:                 app=app
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=app
Replicas:               1 desired | 1 updated | 1 total | 0 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=app
  Containers:
   nginx:
    Image:        nginx:alpine
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      False   MinimumReplicasUnavailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:  <none>
NewReplicaSet:   app-5646649cc9 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  82s   deployment-controller  Scaled up replica set app-5646649cc9 to 1
  
  

 k describe pod app-5646649cc9-m8pv2 
Name:             app-5646649cc9-m8pv2
Namespace:        default
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=app
                  pod-template-hash=5646649cc9
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/app-5646649cc9
Containers:
  nginx:
    Image:        nginx:alpine
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lfpcz (ro)
Volumes:
  kube-api-access-lfpcz:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


cat /etc/kubernetes/manifests/kube-scheduler.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    image: registry.k8s.io/kube-scheduler:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
status: {}


k scale deploy app --replicas=2

k describe deploy app 
Name:                   app
Namespace:              default
CreationTimestamp:      Sat, 25 May 2024 14:06:28 +0000
Labels:                 app=app
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=app
Replicas:               2 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=app
  Containers:
   nginx:
    Image:        nginx:alpine
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   app-5646649cc9 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  16m   deployment-controller  Scaled up replica set app-5646649cc9 to 1
  
  

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-controller-manager:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
		scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
	name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/WRONG-PKI-DIRECTORY
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
	  type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}



kubectl describe node worker-1

top

df -h

service kubelet status

sudo journalctl -u kubectl 

openssl x509 -in /var/lib/kubelet/worker-1.crt -text



kubectl describe nodes node01
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {"VNI":1,"VtepMAC":"e2:27:ca:5a:31:b2"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 192.25.193.6
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 27 May 2024 12:55:20 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  node01
  AcquireTime:     <unset>
  RenewTime:       Mon, 27 May 2024 12:59:05 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----                 ------    -----------------                 ------------------                ------              -------
  NetworkUnavailable   False     Mon, 27 May 2024 12:55:27 +0000   Mon, 27 May 2024 12:55:27 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Mon, 27 May 2024 12:55:51 +0000   Mon, 27 May 2024 12:59:50 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Mon, 27 May 2024 12:55:51 +0000   Mon, 27 May 2024 12:59:50 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Mon, 27 May 2024 12:55:51 +0000   Mon, 27 May 2024 12:59:50 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Mon, 27 May 2024 12:55:51 +0000   Mon, 27 May 2024 12:59:50 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  192.25.193.6
  Hostname:    node01
Capacity:
  cpu:                36
  ephemeral-storage:  1016057248Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214587052Ki
  pods:               110
Allocatable:
  cpu:                36
  ephemeral-storage:  936398358207
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             214484652Ki
  pods:               110
System Info:
  Machine ID:                 49e48c9673ca44dd919fd32b36f0e237
  System UUID:                f27b8c4f-18b7-2007-fc27-7f2ce2f8f47b
  Boot ID:                    9c3ce9ea-4f97-430a-9cf3-be81f6b6c370
  Kernel Version:             5.4.0-1106-gcp
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.26
  Kubelet Version:            v1.29.0
  Kube-Proxy Version:         v1.29.0
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (2 in total)
  Namespace                   Name                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                     ------------  ----------  ---------------  -------------  ---
  kube-flannel                kube-flannel-ds-mjch2    100m (0%)     0 (0%)      50Mi (0%)        0 (0%)         5m35s
  kube-system                 kube-proxy-tfgxw         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m35s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  0 (0%)
  memory             50Mi (0%)  0 (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 5m32s                  kube-proxy       
  Normal   Starting                 5m35s                  kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      5m35s                  kubelet          invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  5m35s (x2 over 5m35s)  kubelet          Node node01 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    5m35s (x2 over 5m35s)  kubelet          Node node01 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     5m35s (x2 over 5m35s)  kubelet          Node node01 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  5m35s                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           5m30s                  node-controller  Node node01 event: Registered Node node01 in Controller
  Normal   NodeReady                5m30s                  kubelet          Node node01 status is now: NodeReady
  Normal   NodeNotReady             65s                    node-controller  Node node01 status is now: NodeNotReady
  
  
ssh node01

service kubelet status

service kubelet start

service kubelet status

#kubelet picks the configuration from this location.
ls /var/lib/kubelet

ls /etc/kubernetes/pki/ca.crt

cat /var/lib/kubelet/config.yaml 

apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s




-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Network Troubleshooting
Network Plugin in Kubernetes

--------------------

There are several plugins available and these are some.


1. Weave Net:


To install,


kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml


You can find details about the network plugins in the following documentation :

https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy


2. Flannel :


 To install,

 

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

   

Note: As of now flannel does not support kubernetes network policies.


3. Calico :

   

   To install,

   curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O

  Apply the manifest using the following command.

      kubectl apply -f calico.yaml

   Calico is said to have most advanced cni network plugin.


In CKA and CKAD exam, you won't be asked to install the CNI plugin. But if asked you will be provided with the exact URL to install it.

Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.



DNS in Kubernetes
-----------------

Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.


Memory and Pods

In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance.


Kubernetes resources for coreDNS are:   

    a service account named coredns,

    cluster-roles named coredns and kube-dns

    clusterrolebindings named coredns and kube-dns, 

    a deployment named coredns,

    a configmap named coredns and a

    service named kube-dns.


While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap.


Port 53 is used for for DNS resolution.


        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }


This is the backend to k8s for cluster.local and reverse domains.


proxy . /etc/resolv.conf


Forward out of cluster domains directly to right authoritative DNS server.



Troubleshooting issues related to coreDNS

1. If you find CoreDNS pods in pending state first check network plugin is installed.

2. coredns pods have CrashLoopBackOff or Error state

If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options:

a)Upgrade to a newer version of Docker.

b)Disable SELinux.

c)Modify the coredns deployment to set allowPrivilegeEscalation to true:


    kubectl -n kube-system get deployment coredns -o yaml | \
      sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
      kubectl apply -f -

d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.


  There are many ways to work around this issue, some are listed here:


    Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the "real" resolv.conf, although this can be different depending on your distribution.

    Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.

    A quick fix is to edit your Corefile, replacing forward . /etc/resolv.conf with the IP address of your upstream DNS, for example forward . 8.8.8.8. But this only fixes the issue for CoreDNS, kubelet will continue to forward the invalid resolv.conf to all default dnsPolicy Pods, leaving them unable to resolve DNS.


3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.

              kubectl -n kube-system get ep kube-dns

If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.



Kube-Proxy
---------

kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.


In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.


kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods.


If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container.


        Command:
          /usr/local/bin/kube-proxy
          --config=/var/lib/kube-proxy/config.conf
          --hostname-override=$(NODE_NAME)

 

    So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running.

 

  In the config file we define the clusterCIDR, kubeproxy mode, ipvs, iptables, bindaddress, kube-config etc.

 
Troubleshooting issues related to kube-proxy

1. Check kube-proxy pod in the kube-system namespace is running.

2. Check kube-proxy logs.

3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.

4. kube-config is defined in the config map.

5. check kube-proxy is running inside the container

    # netstat -plan | grep kube-proxy
    tcp        0      0 0.0.0.0:30081           0.0.0.0:*               LISTEN      1/kube-proxy
    tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      1/kube-proxy
    tcp        0      0 172.17.0.12:33706       172.17.0.12:6443        ESTABLISHED 1/kube-proxy
    tcp6       0      0 :::10256                :::*                    LISTEN      1/kube-proxy



References:

Debug Service issues:

                     https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/

DNS Troubleshooting:

                     https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/
					 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

kubectl expose pod messaging --port 6379 --name messaging-service

# Create a deployment named hr-web-app using the image kodekloud/webapp-color with 2 replicas
kubectl create deployment hr-web-app --image=kodekloud/webapp-color --replicas=2

#Create a static pod named static-busybox on the controlplane node that uses the busybox image and the command sleep 1000
kubectl run static-busybox --image=busybox --dry-run=client -o yaml --command -- sleep 1000

kubectl run static-busybox --image=busybox --dry-run=client -o yaml --command -- sleep 1000 > static-busybox.yaml

cat static-busybox.yaml

mv static-busybox.yaml /etc/kubernetes/manifests/

#Create a POD in the finance namespace named temp-bus with the image redis:alpine.
kubectl run temp-bus --image=redis:alpine -n finance

#Expose the hr-web-app as service hr-web-app-service application on port 30082 on the nodes on the cluster. The web application listens on port 8080
kubectl expose deploy hr-web-app --name=hr-web-app-service --type NodePort --port 8080

kubectl edit svc hr-web-app-service

#Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os_x43kj56.txt. The osImages are under the nodeInfo section under status of each node

kubectl get nodes -o json
{
    "apiVersion": "v1",
    "items": [
        {
            "apiVersion": "v1",
            "kind": "Node",
            "metadata": {
                "annotations": {
                    "flannel.alpha.coreos.com/backend-data": "{\"VNI\":1,\"VtepMAC\":\"46:86:c6:a5:70:e9\"}",
                    "flannel.alpha.coreos.com/backend-type": "vxlan",
                    "flannel.alpha.coreos.com/kube-subnet-manager": "true",
                    "flannel.alpha.coreos.com/public-ip": "192.25.142.9",
                    "kubeadm.alpha.kubernetes.io/cri-socket": "unix:///var/run/containerd/containerd.sock",
                    "node.alpha.kubernetes.io/ttl": "0",
                    "volumes.kubernetes.io/controller-managed-attach-detach": "true"
                },
                "creationTimestamp": "2024-06-11T11:48:55Z",
                "labels": {
                    "beta.kubernetes.io/arch": "amd64",
                    "beta.kubernetes.io/os": "linux",
                    "kubernetes.io/arch": "amd64",
                    "kubernetes.io/hostname": "controlplane",
                    "kubernetes.io/os": "linux",
                    "node-role.kubernetes.io/control-plane": "",
                    "node.kubernetes.io/exclude-from-external-load-balancers": ""
                },
                "name": "controlplane",
                "resourceVersion": "6574",
                "uid": "a04c24dc-c476-49c4-971c-b3e6fb41460d"
            },
            "spec": {
                "podCIDR": "10.244.0.0/24",
                "podCIDRs": [
                    "10.244.0.0/24"
                ]
            },
            "status": {
                "addresses": [
                    {
                        "address": "192.25.142.9",
                        "type": "InternalIP"
                    },
                    {
                        "address": "controlplane",
                        "type": "Hostname"
                    }
                ],
                "allocatable": {
                    "cpu": "36",
                    "ephemeral-storage": "936398358207",
                    "hugepages-1Gi": "0",
                    "hugepages-2Mi": "0",
                    "memory": "214484656Ki",
                    "pods": "110"
                },
                "capacity": {
                    "cpu": "36",
                    "ephemeral-storage": "1016057248Ki",
                    "hugepages-1Gi": "0",
                    "hugepages-2Mi": "0",
                    "memory": "214587056Ki",
                    "pods": "110"
                },
                "conditions": [
                    {
                        "lastHeartbeatTime": "2024-06-11T11:49:22Z",
                        "lastTransitionTime": "2024-06-11T11:49:22Z",
                        "message": "Flannel is running on this node",
                        "reason": "FlannelIsUp",
                        "status": "False",
                        "type": "NetworkUnavailable"
                    },
                    {
                        "lastHeartbeatTime": "2024-06-11T13:05:44Z",
                        "lastTransitionTime": "2024-06-11T11:48:51Z",
                        "message": "kubelet has sufficient memory available",
                        "reason": "KubeletHasSufficientMemory",
                        "status": "False",
                        "type": "MemoryPressure"
                    },
                    {
                        "lastHeartbeatTime": "2024-06-11T13:05:44Z",
                        "lastTransitionTime": "2024-06-11T11:48:51Z",
                        "message": "kubelet has no disk pressure",
                        "reason": "KubeletHasNoDiskPressure",
                        "status": "False",
                        "type": "DiskPressure"
                    },
                    {
                        "lastHeartbeatTime": "2024-06-11T13:05:44Z",
                        "lastTransitionTime": "2024-06-11T11:48:51Z",
                        "message": "kubelet has sufficient PID available",
                        "reason": "KubeletHasSufficientPID",
                        "status": "False",
                        "type": "PIDPressure"
                    },
                    {
                        "lastHeartbeatTime": "2024-06-11T13:05:44Z",
                        "lastTransitionTime": "2024-06-11T11:49:19Z",
                        "message": "kubelet is posting ready status",
                        "reason": "KubeletReady",
                        "status": "True",
                        "type": "Ready"
                    }
                ],
                "daemonEndpoints": {
                    "kubeletEndpoint": {
                        "Port": 10250
                    }
                },
                "images": [
                    {
                        "names": [
                            "docker.io/kodekloud/fluent-ui-running@sha256:78fd68ba8a79adcd3e58897a933492886200be513076ba37f843008cc0168f81",
                            "docker.io/kodekloud/fluent-ui-running:latest"
                        ],
                        "sizeBytes": 389734636
                    },
                    {
                        "names": [
                            "docker.io/library/nginx@sha256:ed6d2c43c8fbcd3eaa44c9dab6d94cb346234476230dc1681227aa72d07181ee",
                            "docker.io/library/nginx:latest"
                        ],
                        "sizeBytes": 70991807
                    },
                    {
                        "names": [
                            "registry.k8s.io/etcd@sha256:22f892d7672adc0b9c86df67792afdb8b2dc08880f49f669eaaa59c47d7908c2",
                            "registry.k8s.io/etcd:3.5.10-0"
                        ],
                        "sizeBytes": 56649232
                    },
                    {
                        "names": [
                            "registry.k8s.io/kube-apiserver@sha256:921d9d4cda40bd481283375d39d12b24f51281682ae41f6da47f69cb072643bc",
                            "registry.k8s.io/kube-apiserver:v1.29.0"
                        ],
                        "sizeBytes": 35068125
                    },
                    {
                        "names": [
                            "registry.k8s.io/kube-controller-manager@sha256:d1e38ea25b27e57b41995ef59ad76dd33481853a5b8d1a91abb7a8be32b7e7da",
                            "registry.k8s.io/kube-controller-manager:v1.29.0"
                        ],
                        "sizeBytes": 33426031
                    },
                    {
                        "names": [
                            "docker.io/kodekloud/webapp-color@sha256:99c3821ea49b89c7a22d3eebab5c2e1ec651452e7675af243485034a72eb1423",
                            "docker.io/kodekloud/webapp-color:latest"
                        ],
                        "sizeBytes": 31777918
                    },
                    {
                        "names": [
                            "docker.io/weaveworks/weave-kube@sha256:d797338e7beb17222e10757b71400d8471bdbd9be13b5da38ce2ebf597fb4e63",
                            "docker.io/weaveworks/weave-kube:2.8.1"
                        ],
                        "sizeBytes": 30924173
                    },
                    {
                        "names": [
                            "registry.k8s.io/kube-proxy@sha256:8da4de35c4929411300eb8052fdfd34095b6090ed0c8dbc776d58bf1c61a2c89",
                            "registry.k8s.io/kube-proxy:v1.29.0"
                        ],
                        "sizeBytes": 28358954
                    },
                    {
                        "names": [
                            "docker.io/flannel/flannel@sha256:c951947891d7811a4da6bf6f2f4dcd09e33c6e1eb6a95022f3f621d00ed4615e",
                            "docker.io/flannel/flannel:v0.23.0"
                        ],
                        "sizeBytes": 28051548
                    },
                    {
                        "names": [
                            "docker.io/library/nginx@sha256:fdbfdaea4fc323f44590e9afeb271da8c345a733bf44c4ad7861201676a95f42",
                            "docker.io/library/nginx:alpine"
                        ],
                        "sizeBytes": 20461204
                    },
                    {
                        "names": [
                            "registry.k8s.io/kube-scheduler@sha256:5df310234e4f9463b15d166778d697830a51c0037ff28a1759daaad2d3cde991",
                            "registry.k8s.io/kube-scheduler:v1.29.0"
                        ],
                        "sizeBytes": 18521870
                    },
                    {
                        "names": [
                            "registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1",
                            "registry.k8s.io/coredns/coredns:v1.11.1"
                        ],
                        "sizeBytes": 18182961
                    },
                    {
                        "names": [
                            "docker.io/library/redis@sha256:0389bb8416d7c6ed065c25745179bf5d358e5d9472dd30a687ab36ffbb650262",
                            "docker.io/library/redis:alpine"
                        ],
                        "sizeBytes": 16799969
                    },
                    {
                        "names": [
                            "registry.k8s.io/coredns/coredns@sha256:a0ead06651cf580044aeb0a0feba63591858fb2e43ade8c9dea45a6a89ae7e5e",
                            "registry.k8s.io/coredns/coredns:v1.10.1"
                        ],
                        "sizeBytes": 16190758
                    },
                    {
                        "names": [
                            "docker.io/weaveworks/weave-npc@sha256:38d3e30a97a2260558f8deb0fc4c079442f7347f27c86660dbfc8ca91674f14c",
                            "docker.io/weaveworks/weave-npc:2.8.1"
                        ],
                        "sizeBytes": 12814131
                    },
                    {
                        "names": [
                            "docker.io/flannel/flannel-cni-plugin@sha256:ca6779c6ad63b77af8a00151cefc08578241197b9a6fe144b0e55484bc52b852",
                            "docker.io/flannel/flannel-cni-plugin:v1.2.0"
                        ],
                        "sizeBytes": 3879095
                    },
                    {
                        "names": [
                            "docker.io/library/busybox@sha256:9ae97d36d26566ff84e8893c64a6dc4fe8ca6d1144bf5b87b2b85a32def253c7",
                            "docker.io/library/busybox:latest"
                        ],
                        "sizeBytes": 2160406
                    },
                    {
                        "names": [
                            "docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966"
                        ],
                        "sizeBytes": 2160005
                    },
                    {
                        "names": [
                            "docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6df416dea4f41046e0f37d47",
                            "docker.io/library/busybox:1.28"
                        ],
                        "sizeBytes": 727869
                    },
                    {
                        "names": [
                            "registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097",
                            "registry.k8s.io/pause:3.9"
                        ],
                        "sizeBytes": 321520
                    },
                    {
                        "names": [
                            "registry.k8s.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db",
                            "registry.k8s.io/pause:3.6"
                        ],
                        "sizeBytes": 301773
                    }
                ],
                "nodeInfo": {
                    "architecture": "amd64",
                    "bootID": "6dcca176-bbe2-47c0-b0c8-5cc189ac3c63",
                    "containerRuntimeVersion": "containerd://1.6.26",
                    "kernelVersion": "5.4.0-1106-gcp",
                    "kubeProxyVersion": "v1.29.0",
                    "kubeletVersion": "v1.29.0",
                    "machineID": "9b149c90c227409fb12873ee061654c4",
                    "operatingSystem": "linux",
                    "osImage": "Ubuntu 22.04.4 LTS",
                    "systemUUID": "bf88a9e6-55ee-0168-e1d9-c24ba67ab2be"
                }
            }
        }
    ],
    "kind": "List",
    "metadata": {
        "resourceVersion": ""
    }
}


##---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Take a backup of the etcd cluster and save it to /opt/etcd-backup.db.

cat /etc/kubernetes/manifests/etcd.yaml

cat /etc/kubernetes/manifests/etcd.yaml | grep file

ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379  snapshot save /opt/etcd-backup.db --cacert=/etc/kubernetes/pki/e
tcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key

#Create a Pod called redis-storage with image: redis:alpine with a Volume of type emptyDir that lasts for the life of the Pod.
#Specs on the below.
#Pod named 'redis-storage' created
#Pod 'redis-storage' uses Volume type of emptyDir
#Pod 'redis-storage' uses volumeMount with mountPath = /data/redis

kubectl run redis-storage --image=redis:alpine --dry-run=client -o yaml > redis-storage.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis-storage
  name: redis-storage
spec:
  containers:
  - image: redis:alpine
    name: redis-storage
    resources: {}
    volumeMounts:
    - mountPath: /data/redis
      name: cache-volume
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
  - name: cache-volume
    emptyDir: {}
status: {}

kubectl create -f redis-storage.yaml 

#Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time.
#The container should sleep for 4800 seconds

 vi super-user-pod.yaml 
 apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: super-user-pod
  name: super-user-pod
spec:
  containers:
  - image: busybox:1.28
    name: super-user-pod
    command: ["sleep","4800"]  
    resources: {}
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "SYS_TIME"]  
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl create -f super-user-pod.yaml 

#A pod definition file is created at /root/CKA/use-pv.yaml. Make use of this manifest file and mount the persistent volume called pv-1. Ensure the pod is running and the PV is bound.
#mountPath: /data
#persistentVolumeClaim Name: my-pvc 

vi pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
	  
	  
kubectl create -f pvc.yaml

vi /root/CKA/use-pv.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image: nginx
    name: use-pv
    resources: {}
    volumeMounts:
      - mountPath: "/data"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: my-pvc
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl create -f /root/CKA/use-pv.yaml 

#Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update.

kubectl create deploy nginx-deploy --image=nginx:1.16 --replicas=1

kubectl get deploy

 kubectl set image deployment/nginx-deploy nginx=nginx:1.17
 
 kubectl describe deploy nginx-deploy 
 
#Create a new user called john. Grant him access to the cluster. John should have permission to create, list, get, update and delete pods in the development namespace . The private #key exists in the location: /root/CKA/john.key and csr at /root/CKA/john.csr.
#Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName.
#Please refer the documentation to see an example. The documentation tab is available at the top right of terminal.
#CSR: john-developer Status:Approved
#Role Name: developer, namespace: development, Resource: Pods
#Access: User 'john' has appropriate permissions

vi csr.yaml

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZVzVuWld4aE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTByczhJTHRHdTYxakx2dHhWTTJSVlRWMDNHWlJTWWw0dWluVWo4RElaWjBOCnR2MUZtRVFSd3VoaUZsOFEzcWl0Qm0wMUFSMkNJVXBGd2ZzSjZ4MXF3ckJzVkhZbGlBNVhwRVpZM3ExcGswSDQKM3Z3aGJlK1o2MVNrVHF5SVBYUUwrTWM5T1Nsbm0xb0R2N0NtSkZNMUlMRVI3QTVGZnZKOEdFRjJ6dHBoaUlFMwpub1dtdHNZb3JuT2wzc2lHQ2ZGZzR4Zmd4eW8ybmlneFNVekl1bXNnVm9PM2ttT0x1RVF6cXpkakJ3TFJXbWlECklmMXBMWnoyalVnald4UkhCM1gyWnVVV1d1T09PZnpXM01LaE8ybHEvZi9DdS8wYk83c0x0MCt3U2ZMSU91TFcKcW90blZtRmxMMytqTy82WDNDKzBERHk5aUtwbXJjVDBnWGZLemE1dHJRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBR05WdmVIOGR4ZzNvK21VeVRkbmFjVmQ1N24zSkExdnZEU1JWREkyQTZ1eXN3ZFp1L1BVCkkwZXpZWFV0RVNnSk1IRmQycVVNMjNuNVJsSXJ3R0xuUXFISUh5VStWWHhsdnZsRnpNOVpEWllSTmU3QlJvYXgKQVlEdUI5STZXT3FYbkFvczFqRmxNUG5NbFpqdU5kSGxpT1BjTU1oNndLaTZzZFhpVStHYTJ2RUVLY01jSVUyRgpvU2djUWdMYTk0aEpacGk3ZnNMdm1OQUxoT045UHdNMGM1dVJVejV4T0dGMUtCbWRSeEgvbUNOS2JKYjFRQm1HCkkwYitEUEdaTktXTU0xMzhIQXdoV0tkNjVoVHdYOWl4V3ZHMkh4TG1WQzg0L1BHT0tWQW9FNkpsYWFHdTlQVmkKdjlOSjVaZlZrcXdCd0hKbzZXdk9xVlA3SVFjZmg3d0drWm89Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
  
cat john.csr | base64 | tr -d "\n"

vi csr.yaml


apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:  request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQUxHQTVTM3FiV0NiMkRheEExNjdQanE1N3REZGFtV1R1NmJJS1Y3Q25RdjBzbDM4Ckl6Nlc3RFd0WnJiUnY4Z3BkK01sRjI2aDZpbmFGeXozeFFNQVdvQjVXUFFEU2xEbzVIWm9RNWNlSnpFMXRTcEwKOC9QOS9YQWR2SVByUUsvUmdJeEpqa3loUkdnUjdzYzZSZ3IzczMyWU0vQ3pLbUVrWVVVWWo4TndxWi9QVno2WApoSmU0NGZ4SVJQTWU4TzVWKy9FNnp5Z25rajQ5dGlFSG9XZE5FOTV3aXlhU2pLazdHSm1hQnNGNEl0M1lYc1BmCjFGbjZxeVptaFpkRytsT3J6bS9VNGdmQ2FZMDY2alVwMzNGaWlyeHBydnpnKzh5SkpFVHhTN1Bqc2V2VGVYcisKNlg2RlBRVjJ1VDV2dk1McVQwdE5pYkhLNklIRzlzWXZFTytHd25VQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQTQ3ZXUydkl1ZXJuWlRrS085dGF5dGlibWY1bFFoZFFNQnF5N0F1cHVIOTdhUTU5VnRJbWQ3CngyZGU2YVJlZFJkd3FWWE1KbGdQOHl6NHRsbm50czY1L0F2WVVWZnRhd3U5cGFPSHJncFZraXBFWVF0bHBJS2kKVGxBcWpGUDlVQVFuMkJxK0lxektxaUIyazN5YXhiN2gvTjFpNDNDM21LdXlUTnRNODk2MitKRTExRVNBaWJsTAo5KzArb0FmU0xWWFhySXBkY0RRbldOeWpIaVVaWGlLVG51Sko0WEdCYjY3YjcvQzdOU0dnczZoWDUzbUlNS09nCnl6MU4zVm5WNkFrT3Q2ZDdSd2oxZ1c4bFlPdWI3TnUyN253UERjSm8xMUNJc1BjOTZtK3RyZE9QQjhkZTB1VFkKY3ZES3hZNnNzZFlWK3BsanBhTnBNUlVjblF2VnpRMnQKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg==
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth
  
kubectl create -f csr.yaml

kubectl get csr

kubectl certificate approve john-developer

kubectl create role developer --verb=create,get,list,update,delete --resource=pods -n development

kubectl create rolebinding john-developer --role=developer --user=john -n development 

kubectl delete rolebinding john-developer -n development

kubectl auth can-i get pods -n development --as john

#Create a nginx pod called nginx-resolver using image nginx, expose it internally with a service called nginx-resolver-service. Test that you are able to look up the service and pod #names from within the cluster. Use the image: busybox:1.28 for dns lookup. Record results in /root/CKA/nginx.svc and /root/CKA/nginx.pod
#Pod: nginx-resolver created
#Service DNS Resolution recorded correctly
#Pod DNS resolution recorded correctly


kubectl run nginx-resolver --image=nginx

kubectl expose pod nginx-resolver --name=nginx-resolver-service --port=80

kubectl run busybox --image=busybox:1.28 -- sleep 4000

kubectl exec busybox -- nslookup nginx-resolver-service

kubectl exec busybox -- nslookup nginx-resolver-service > /root/CKA/nginx.svc

kubectl exec busybox -- nslookup 10-244-192-1.default.pod.cluster.local > /root/CKA/nginx.pod

#Create a static pod on node01 called nginx-critical with image nginx and make sure that it is recreated/restarted automatically in case of a failure.
#Use /etc/kubernetes/manifests as the Static Pod path for example.
#static pod configured under /etc/kubernetes/manifests ?
#Pod nginx-critical-node01 is up and running

 kubectl run nginx-critical --image=nginx --restart=Always --dry-run=client -o yaml
 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx-critical
  name: nginx-critical
spec:
  containers:
  - image: nginx
    name: nginx-critical
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

#Create a new service account with the name pvviewer. Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role called #pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
#Next, create a pod called pvviewer with the image: redis and serviceAccount: pvviewer in the default namespace.
#ServiceAccount: pvviewer
#ClusterRole: pvviewer-role
#ClusterRoleBinding: pvviewer-role-binding
#Pod: pvviewer
#Pod configured to use ServiceAccount pvviewer ?


kubectl create serviceaccount pvviewer

kubectl create clusterrole pvviewer-role --verb=list --resource=persistentVolumes

kubectl get clusterrole pvviewer-role 

kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer

kubectl describe clusterrolebinding pvviewer-role-binding

k run pvviewer --image=redis --dry-run=client -o yaml > pvviewer.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pvviewer
  name: pvviewer
spec:
  serviceAccountName: pvviewer
  containers:
  - image: redis
    name: pvviewer
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl describe pod pvviewer | grep -i service

#List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.
#Answer should be in the format: InternalIP of controlplane<space>InternalIP of node01 (in a single line)

kubectl get nodes -o json | jq -c 'paths' | grep type

kubectl get nodes -o json | jq -c 'paths' | grep type | grep -v conditions

kubectl get nodes -o jsonpath='{.items[*]}' | jq

kubectl get nodes -o jsonpath='{.items[0].status.addresses}' | jq

kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")]}' | jq

kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}'

kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips

#Create a pod called multi-pod with two containers.
#Container 1: name: alpha, image: nginx
#Container 2: name: beta, image: busybox, command: sleep 4800
#Environment Variables:
#container 1:
#name: alpha
#Container 2:
#name: beta
#Pod Name: multi-pod
#Container 1: alpha
#Container 2: beta
#Container beta commands set correctly?
#Container 1 Environment Value Set
#Container 2 Environment Value Set

kubectl run multi-pod --image=nginx --dry-run=client -o yaml > multi-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: multi-pod
  name: multi-pod
spec:
  containers:
  - image: nginx
    name: alpha
    env:
      - name: "name"
        value: "alpha"
  - image: busybox
    name: beta
    command:
      - sleep
      - "4800"
    env:
      - name: "name"
        value: "beta"
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl apply -f multi-pod.yaml

#Create a Pod called non-root-pod , image: redis:alpine
#runAsUser: 1000
#fsGroup: 2000
#Pod non-root-pod fsGroup configured
#Pod non-root-pod runAsUser configured


kubectl run non-root-pod --image=redis:alpine --dry-run=client -o yaml > non-root-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  containers:
  - image: redis:alpine
    name: non-root-pod
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl apply -f non-root-pod.yaml 

kubectl get pods

kubectl get pod non-root-pod -o yaml

apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"creationTimestamp":null,"labels":{"run":"non-root-pod"},"name":"non-root-pod","namespace":"default"},"spec":{"containers":[{"image":"redis:alpine","name":"non-root-pod","resources":{}}],"dnsPolicy":"ClusterFirst","restartPolicy":"Always","securityContext":{"fsGroup":2000,"runAsUser":1000}},"status":{}}
  creationTimestamp: "2024-06-17T11:11:26Z"
  labels:
    run: non-root-pod
  name: non-root-pod
  namespace: default
  resourceVersion: "4043"
  uid: 5553a876-3bb2-4d36-92db-92be9f1eafd6
spec:
  containers:
  - image: redis:alpine
    imagePullPolicy: IfNotPresent
    name: non-root-pod
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-5zrjc
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: node01
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    fsGroup: 2000
    runAsUser: 1000
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-5zrjc
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-06-17T11:11:32Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2024-06-17T11:11:26Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-06-17T11:11:32Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-06-17T11:11:32Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-06-17T11:11:26Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://aed539e4bd13d3065ca937d4e90c245675d13726fdf76aad0b4e43efab9316bf
    image: docker.io/library/redis:alpine
    imageID: docker.io/library/redis@sha256:0389bb8416d7c6ed065c25745179bf5d358e5d9472dd30a687ab36ffbb650262
    lastState: {}
    name: non-root-pod
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-17T11:11:32Z"
  hostIP: 192.18.80.6
  hostIPs:
  - ip: 192.18.80.6
  phase: Running
  podIP: 10.244.192.2
  podIPs:
  - ip: 10.244.192.2
  qosClass: BestEffort
  startTime: "2024-06-17T11:11:26Z"
  
#We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it.
#Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80.
#Important: Don't delete any current objects deployed.
#Important: Don't Alter Existing Objects!
#NetworkPolicy: Applied to All sources (Incoming traffic from all pods)?
#NetWorkPolicy: Correct Port?
#NetWorkPolicy: Applied to correct Pod?

kubectl run curl --image=alpine/curl --rm -it -- sh
# test this command. It hangs and no reply
# curl np-test-service

vi network-policy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  policyTypes:
  - Ingress
  ingress:
  - 
    ports:
    - protocol: TCP
      port: 80


kubectl apply -f network-policy.yaml

kubectl get networkpolicy

#Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine, to ensure workloads are not scheduled to this worker node. Finally, #create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01.
#key: env_type, value: production, operator: Equal and effect: NoSchedule
#Key = env_type
#Value = production
#Effect = NoSchedule
#pod 'dev-redis' (no tolerations) is not scheduled on node01?
#Create a pod 'prod-redis' to run on node01

kubectl taint nodes node01 env_type=production:NoSchedule

kubectl run dev-redis --image=redis:alpine

kubectl get pod -o wide

kubectl run prod-redis --image=redis:alpine --dry-run=client -o yaml > prod-redis.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: prod-redis
  name: prod-redis
spec:
  tolerations:
  - key: "env_type"
    operator: "Equal"
    value: "production"
    effect: "NoSchedule"
  containers:
  - image: redis:alpine
    name: prod-redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

kubectl apply -f prod-redis.yaml

#Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier .
#image: redis:alpine
#Use appropriate labels and create all the required objects if it does not exist in the system already.
#hr-pod labeled with environment production?
#hr-pod labeled with tier frontend?

kubectl create ns hr

kubectl run hr-pod -n hr --image=redis:alpine --labels="environment=production,tier=frontend"


#A kubeconfig file called super.kubeconfig has been created under /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it.
#Fix /root/CKA/super.kubeconfig

kubectl get nodes --kubeconfig /root/CKA/super.kubeconfig

vi /root/CKA/super.kubeconfig

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJR2k3N28yTGNxeW93RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBMk1UY3hNekF4TXpGYUZ3MHpOREEyTVRVeE16QTJNekZhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURwQjN3K281SEJLSzNCTU84MW5MWDZXbWNISmQwV1NLK0g5WjJpZWRjcGpUV1laWDM0Vkcwd0JFNjUKbDZJUVd6RkliUVMyUTdraWp1M3BlM2FxTWZPSFJMN0VicWJWV0NtZlAvaUdpWWNKcm45OFJ6c1MwZ3FOZTJJeQpDSmNrWVFGQUhPSmxVc0dNTEZnR1VON05kMDJ1K2Zld2MzbTJxdTFmNjNUT2phVHBoc3FaSlBZT1pnR0JteXdHClJVcDBqbEdrWWI1bW40YlQ4R29idGFFczFVK1FORnlmbTFyY083SjZJNW9ucGhHQ1lSdmNtWFBleG9lUVpOVWcKRC92bmlQQnRCZTlFd3p5eWYvNzRRUTdvTWJxRVZGU3lLWU5qZmJQeG1SaHlOWnFoaUFDN2lYbWRTMklMMURZLwpHbjNZY3hlRzMwOWI1U0FINVhjaExWTi9EOE1CQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJSWjNVVXdIN2lROXQySG95dFF5TENnZkY0dUl6QVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQ01weG5PVjVOagpkMzRscm5DS0NlRElrcFpiRCtPLytzNlRNMFJEYjloNEtSd1ZGLzVRRCtlYjE2SmJUUWZLVm1sd09iVnkzQk5IClc2WkVxNm5QK0ZtTnU4STBCRC9WQ2dTaVJzUmRiYVJ3cm5veEd5Z09mMitMaEEzaDI3czQyRXZndHpHY1lmV3gKZnVLb25lQjdUMlpXTmZKZ3JiN2t3MFhLVGtXcGxnL1JlWkdQOWFlWXBVaW8xTGJmTnhpQ3c1QlpJaHZlSWFIMgo5NW5jN0VnbkNjZzBXQ3NLaW4vaVVIRzRoaVJqZTdnS2N2WEFFM1JORnZscTNMbk1HbXYrRDl3NGdSTi96ZGRuCkUxeE0weDhnR2dJN3FBcGtFN0dSRWg1bmFoZE5mYUVMbW5iK1ZSM0dSU2dFNU1jYWdoSTVIMHZJVFVqYTdmUzEKUHhiMTRlSjVUamJtCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://controlplane:9999
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
  
cat .kube/config

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJR2k3N28yTGNxeW93RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBMk1UY3hNekF4TXpGYUZ3MHpOREEyTVRVeE16QTJNekZhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURwQjN3K281SEJLSzNCTU84MW5MWDZXbWNISmQwV1NLK0g5WjJpZWRjcGpUV1laWDM0Vkcwd0JFNjUKbDZJUVd6RkliUVMyUTdraWp1M3BlM2FxTWZPSFJMN0VicWJWV0NtZlAvaUdpWWNKcm45OFJ6c1MwZ3FOZTJJeQpDSmNrWVFGQUhPSmxVc0dNTEZnR1VON05kMDJ1K2Zld2MzbTJxdTFmNjNUT2phVHBoc3FaSlBZT1pnR0JteXdHClJVcDBqbEdrWWI1bW40YlQ4R29idGFFczFVK1FORnlmbTFyY083SjZJNW9ucGhHQ1lSdmNtWFBleG9lUVpOVWcKRC92bmlQQnRCZTlFd3p5eWYvNzRRUTdvTWJxRVZGU3lLWU5qZmJQeG1SaHlOWnFoaUFDN2lYbWRTMklMMURZLwpHbjNZY3hlRzMwOWI1U0FINVhjaExWTi9EOE1CQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJSWjNVVXdIN2lROXQySG95dFF5TENnZkY0dUl6QVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQ01weG5PVjVOagpkMzRscm5DS0NlRElrcFpiRCtPLytzNlRNMFJEYjloNEtSd1ZGLzVRRCtlYjE2SmJUUWZLVm1sd09iVnkzQk5IClc2WkVxNm5QK0ZtTnU4STBCRC9WQ2dTaVJzUmRiYVJ3cm5veEd5Z09mMitMaEEzaDI3czQyRXZndHpHY1lmV3gKZnVLb25lQjdUMlpXTmZKZ3JiN2t3MFhLVGtXcGxnL1JlWkdQOWFlWXBVaW8xTGJmTnhpQ3c1QlpJaHZlSWFIMgo5NW5jN0VnbkNjZzBXQ3NLaW4vaVVIRzRoaVJqZTdnS2N2WEFFM1JORnZscTNMbk1HbXYrRDl3NGdSTi96ZGRuCkUxeE0weDhnR2dJN3FBcGtFN0dSRWg1bmFoZE5mYUVMbW5iK1ZSM0dSU2dFNU1jYWdoSTVIMHZJVFVqYTdmUzEKUHhiMTRlSjVUamJtCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://controlplane:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURLVENDQWhHZ0F3SUJBZ0lJTW43NWtLeHc0dTB3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBMk1UY3hNekF4TXpGYUZ3MHlOVEEyTVRjeE16QTJNek5hTUR3eApIekFkQmdOVkJBb1RGbXQxWW1WaFpHMDZZMngxYzNSbGNpMWhaRzFwYm5NeEdUQVhCZ05WQkFNVEVHdDFZbVZ5CmJtVjBaWE10WVdSdGFXNHdnZ0VpTUEwR0NTcUdTSWIzRFFFQkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDNzdkVSsKYzNSTzZMeklyb0haQU1zZmRHd1NJa2ZsQTB5TUxMWmR6Ym5wTnFYbHkyU0cwOFpjaHZEQVVSTUR5NFNsWVJpZQpWMituYTNFY1J4SDZWK29LdHpmWjNDcjVoY3BZdXZRTVRTR1JtOVVtbkxBYUZ3NDRwTEM4WlRvYnl4WndpMXc5CjNPYk85anlMQkNrTmlONHhzK0U1VzhsR0c4dDFaQVlKWkU5ZGErNnlPanNoeGsvbTBZSFJrR1QwWXFpamxoQlIKdDBsendHMG9VKzBvYzA0Q3JLK1E4aUxVU1hlN1c0dnQ3Y3MyVmY1WkVidnJiQ0ovK2dybStUcFd6VENwMitsZQp3R1U3ZFNnZDRWQjE3YUhqK0RLRWt1WjVwRHZNZzR3RWR5Sms4Tm9ZZmR1RTduclgvZm52L0NXeFVHVGNQMm15CjcvL0tXaStkMkJpSHNWSkZBZ01CQUFHalZqQlVNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUsKQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjhHQTFVZEl3UVlNQmFBRkZuZFJUQWZ1SkQyM1llagpLMURJc0tCOFhpNGpNQTBHQ1NxR1NJYjNEUUVCQ3dVQUE0SUJBUUE0dHdtakNiZG1xT1Z2aTdQWTRGMzNONTJMCitjeVNDYXl4aVVpTzg3RENzYWc5dFFHMDRmcDVlUmpzYXpET20yNm5hdkduL1lyL2tObGt4Y3U3QTJIK1owWTYKa0Y3OTNxT2xxQmNoYnRURm8xWXdJTGpmaUVpU2dlY3R2OXA1U25LZUVaK1V3N00wZWRkSzNYSmMwenFxdU5oTwpncGtvS2l0QXRWN09VZzlJZTcxTzA1d1h6bEYrRnIvQ1VLejcvOEhLWHNOV0RYODJ4MnRlM3RXNVZWQ2phTGNRClhQTXVNaEJaYXJnVmhta0daSER3QU5hMFdoK0pCdUJQbGZUa3FrVndZUDhXNDBPQnhiQ04rNG1QRk9KbXVNNFkKemVzUUlQd1ZxZkJmMkZwd2ZtM3NxOC9sZHhhamJ3aG4yQzdBQTBpWWdEU2ZrM0lrVmZZTGVzRnB4azNyCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    client-key-data: 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

#After Editing
vi /root/CKA/super.kubeconfig

apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJR2k3N28yTGNxeW93RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBMk1UY3hNekF4TXpGYUZ3MHpOREEyTVRVeE16QTJNekZhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURwQjN3K281SEJLSzNCTU84MW5MWDZXbWNISmQwV1NLK0g5WjJpZWRjcGpUV1laWDM0Vkcwd0JFNjUKbDZJUVd6RkliUVMyUTdraWp1M3BlM2FxTWZPSFJMN0VicWJWV0NtZlAvaUdpWWNKcm45OFJ6c1MwZ3FOZTJJeQpDSmNrWVFGQUhPSmxVc0dNTEZnR1VON05kMDJ1K2Zld2MzbTJxdTFmNjNUT2phVHBoc3FaSlBZT1pnR0JteXdHClJVcDBqbEdrWWI1bW40YlQ4R29idGFFczFVK1FORnlmbTFyY083SjZJNW9ucGhHQ1lSdmNtWFBleG9lUVpOVWcKRC92bmlQQnRCZTlFd3p5eWYvNzRRUTdvTWJxRVZGU3lLWU5qZmJQeG1SaHlOWnFoaUFDN2lYbWRTMklMMURZLwpHbjNZY3hlRzMwOWI1U0FINVhjaExWTi9EOE1CQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJSWjNVVXdIN2lROXQySG95dFF5TENnZkY0dUl6QVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQ01weG5PVjVOagpkMzRscm5DS0NlRElrcFpiRCtPLytzNlRNMFJEYjloNEtSd1ZGLzVRRCtlYjE2SmJUUWZLVm1sd09iVnkzQk5IClc2WkVxNm5QK0ZtTnU4STBCRC9WQ2dTaVJzUmRiYVJ3cm5veEd5Z09mMitMaEEzaDI3czQyRXZndHpHY1lmV3gKZnVLb25lQjdUMlpXTmZKZ3JiN2t3MFhLVGtXcGxnL1JlWkdQOWFlWXBVaW8xTGJmTnhpQ3c1QlpJaHZlSWFIMgo5NW5jN0VnbkNjZzBXQ3NLaW4vaVVIRzRoaVJqZTdnS2N2WEFFM1JORnZscTNMbk1HbXYrRDl3NGdSTi96ZGRuCkUxeE0weDhnR2dJN3FBcGtFN0dSRWg1bmFoZE5mYUVMbW5iK1ZSM0dSU2dFNU1jYWdoSTVIMHZJVFVqYTdmUzEKUHhiMTRlSjVUamJtCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://controlplane:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
  

kubectl get nodes --kubeconfig /root/CKA/super.kubeconfig

NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   30m   v1.29.0
node01         Ready    <none>          30m   v1.29.0

#We have created a new deployment called nginx-deploy. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it.
# deployment has 3 replicas


kubectl get deploy

kubectl scale deployment nginx-deploy --replicas=3

kubectl get pod -n kube-system

NAME                                   READY   STATUS             RESTARTS      AGE
coredns-69f9c977-bhkpx                 1/1     Running            0             35m
coredns-69f9c977-hnbxd                 1/1     Running            0             35m
etcd-controlplane                      1/1     Running            0             36m
kube-apiserver-controlplane            1/1     Running            0             36m
kube-contro1ler-manager-controlplane   0/1     ImagePullBackOff   0             4m26s
kube-proxy-cc49t                       1/1     Running            0             35m
kube-proxy-dv49h                       1/1     Running            0             35m
kube-scheduler-controlplane            1/1     Running            0             36m
weave-net-89rsz                        2/2     Running            1 (35m ago)   35m
weave-net-vlgb6                        2/2     Running            0             35m

cd /etc/kubernetes/manifests/
vi kube-controller-manager.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-contro1ler-manager
    tier: control-plane
  name: kube-contro1ler-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-contro1ler-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-contro1ler-manager:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
		scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-contro1ler-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
	  name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}


#After Editing
cd /etc/kubernetes/manifests/
vi kube-controller-manager.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-cidr=10.244.0.0/16
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --use-service-account-credentials=true
    image: registry.k8s.io/kube-controller-manager:v1.29.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
		scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
	  name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}



kubectl get pod -n kube-system 

NAME                                   READY   STATUS    RESTARTS      AGE
coredns-69f9c977-bhkpx                 1/1     Running   0             45m
coredns-69f9c977-hnbxd                 1/1     Running   0             45m
etcd-controlplane                      1/1     Running   0             45m
kube-apiserver-controlplane            1/1     Running   0             45m
kube-controller-manager-controlplane   1/1     Running   0             66s
kube-proxy-cc49t                       1/1     Running   0             45m
kube-proxy-dv49h                       1/1     Running   0             44m
kube-scheduler-controlplane            1/1     Running   0             45m
weave-net-89rsz                        2/2     Running   1 (45m ago)   45m
weave-net-vlgb6                        2/2     Running   0             44m




kubectl get deploy

NAME           READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deploy   3/3     3            3           28m


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Kubernetes Update and Project Videos - Your Essential Guide

Uncover additional insights through the videos listed below:
Kubernetes Update Videos
1. Kubernetes v1.27 Update
https://www.youtube.com/watch?v=rUFgZuIp1mY

2. Kubernetes v1.28 Update
https://www.youtube.com/watch?v=mRlBtYc-HSk

3. Kubernetes v1.29 Update
https://www.youtube.com/watch?v=yCkQgKVwSVU


